{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot RSA results + statistics + first/second half\n",
    "\n",
    "- based on the `rsa_analysis_9x9.ipynb` notebook\n",
    "- based on statistics from `clusterperm.py` script\n",
    "\n",
    "---\n",
    "\n",
    "1. Select which model to look at\n",
    "1. Plot the data across sampling conditions\n",
    "1. Find significant clusters\n",
    "1. Plot clusters\n",
    "1. Plot 2x2 \"summaries\" for each significant cluster\n",
    "    1. Repeat for \"first_half\" and \"second_half\" of trials\n",
    "    1. Add statistics for the \"summaries\"\n",
    "    1. Add post-hoc tests for significant statistics\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Also plot the RDMs:\n",
    "\n",
    "- for the significant observed clusters\n",
    "    - per subject\n",
    "        - per task\n",
    "        - averaged over tasks\n",
    "    - averaged over subjects\n",
    "        - per task\n",
    "        - averaged over tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T12:20:03.439148Z",
     "start_time": "2020-10-15T12:20:01.320055Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import os.path as op\n",
    "import itertools\n",
    "import json\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from clusterperm import (\n",
    "    evaluate_significance,\n",
    "    return_observed_clusters,\n",
    "    _return_clusters,\n",
    ")\n",
    "from utils import BIDS_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T12:20:03.443382Z",
     "start_time": "2020-10-15T12:20:03.440727Z"
    }
   },
   "outputs": [],
   "source": [
    "# File path template\n",
    "path_template = op.join(BIDS_ROOT, \"derivatives\", \"rsa_{}\", \"{}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T10:01:21.389905Z",
     "start_time": "2020-10-15T10:01:21.372444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "modelname = \"numberline\"\n",
    "orth = False\n",
    "thresh = 0.05\n",
    "cluster_defining_thresh_ave = 0.05  # cluster defining threshold for average effect\n",
    "clusterstat = \"length\"  # \"length\" or \"mass\"\n",
    "clusterthresh = 0.05\n",
    "rsa_dims = \"9x9\"\n",
    "nth_perm_results = 0  # if multiple perm result distrs, which one to take?\n",
    "do_2x2_with_halfs_ecFalse = False\n",
    "\n",
    "# some more settings for one sample ttest\n",
    "niterations = 10_000\n",
    "y = 0\n",
    "\n",
    "# set njobs to 1 if you do not want to run in parallel\n",
    "njobs = min(multiprocessing.cpu_count() - 1, 25)\n",
    "\n",
    "# which RSA to do the analyses on\n",
    "# This is the folder name of the results nested in the 9x9 ... folder\n",
    "rsa_kind = (\n",
    "    \"cv-False_flip-False_rsa-pearson_dist-euclidean_half-both_exclude-Identity_mnn-FalseFalseFalse_ec-False_c-(0.6, 1.6)t--0.8s-150b-(None, 0)s-True_outcome\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T10:01:21.400356Z",
     "start_time": "2020-10-15T10:01:21.391455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try to find settings that would overwrite the settings here\n",
    "configpath = op.join(BIDS_ROOT, \"code\", \"rsa_plotting_config.json\")\n",
    "if op.exists(configpath):\n",
    "    print(f\"\\n\\nOVERWRITTING SETTINGS USING CONFIG:\\n{configpath}\\n\\n\")\n",
    "    with open(configpath, \"r\") as fin:\n",
    "        config = json.load(fin)\n",
    "    print(config)\n",
    "    rsa_dims = config.get(\"rsa_dims\", rsa_dims)\n",
    "    rsa_kind = config.get(\"rsa_kind\", rsa_kind)\n",
    "    modelname = config.get(\"modelname\", modelname)\n",
    "    orth = config.get(\"orth\", orth)\n",
    "    nth_perm_results = config.get(\"nth_perm_results\", nth_perm_results)\n",
    "    clusterstat = config.get(\"clusterstat\", clusterstat)\n",
    "    do_2x2_with_halfs_ecFalse = config.get(\n",
    "        \"do_2x2_with_halfs_ecFalse\", do_2x2_with_halfs_ecFalse\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T10:01:22.483603Z",
     "start_time": "2020-10-15T10:01:22.294408Z"
    }
   },
   "outputs": [],
   "source": [
    "rsa_folder = path_template.format(rsa_dims, rsa_kind)\n",
    "\n",
    "# Finish formatting file paths\n",
    "results = op.join(rsa_folder, \"rsa_results.csv\")\n",
    "results_first_half = results.replace(\"half-both\", \"half-first\")\n",
    "results_second_half = results.replace(\"half-both\", \"half-second\")\n",
    "\n",
    "if do_2x2_with_halfs_ecFalse:\n",
    "    results_first_half = results_first_half.replace(\"ec-True\", \"ec-False\")\n",
    "    results_second_half = results_second_half.replace(\"ec-True\", \"ec-False\")\n",
    "\n",
    "full_modelname = \"orth\" + modelname if orth else modelname\n",
    "\n",
    "cluster_distr_files = glob.glob(\n",
    "    op.join(\n",
    "        rsa_folder,\n",
    "        \"clusterperm_results_*T*\",\n",
    "        f\"model-{full_modelname}_stat-{clusterstat}_thresh-{thresh}_distr.npy\",\n",
    "    )\n",
    ")\n",
    "\n",
    "if len(cluster_distr_files) == 1:\n",
    "    cluster_distr_file = cluster_distr_files[0]\n",
    "elif len(cluster_distr_files) > 1:\n",
    "    cluster_distr_file = sorted(cluster_distr_files)[nth_perm_results]\n",
    "    print(f\"found multiple cluster perm results distrs: {cluster_distr_files}\")\n",
    "    print(f\"picking index {nth_perm_results}\")\n",
    "else:\n",
    "    raise RuntimeError(\"no clusterperm_results found.\")\n",
    "\n",
    "# For column order, we have a script later that makes sure column order\n",
    "# is always the same, also if we get multiple files.\n",
    "cluster_distr_col_order_file = glob.glob(\n",
    "    op.join(\n",
    "        rsa_folder,\n",
    "        \"clusterperm_results_*T*\",\n",
    "        f\"model-{full_modelname}_stat-{clusterstat}_thresh-{thresh}_distr_column_order.txt\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:37:22.264458Z",
     "start_time": "2020-09-02T08:37:22.258386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Where to save results\n",
    "outfolder = op.join(\n",
    "    rsa_folder, \"perm_and_2x2_outputs\", full_modelname + f\"_{clusterstat}\"\n",
    ")\n",
    "os.makedirs(outfolder, exist_ok=True)\n",
    "\n",
    "# And where to save results for RDM plots\n",
    "model = \"o_\" + modelname if orth else modelname\n",
    "outfolder_rdms = op.join(rsa_folder, \"perm_and_RDM_outputs\", model)\n",
    "os.makedirs(outfolder_rdms, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load RSA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:37:22.816339Z",
     "start_time": "2020-09-02T08:37:22.266266Z"
    }
   },
   "outputs": [],
   "source": [
    "df_rsa = pd.read_csv(results)\n",
    "df_rsa_first_half = pd.read_csv(results_first_half)\n",
    "df_rsa_second_half = pd.read_csv(results_second_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the rsa_method that was used\n",
    "assert (\n",
    "    df_rsa[\"method\"].nunique() == 1\n",
    "), f\"more than one rsa_method detected: {df_rsa['method']}\"\n",
    "rsa_method = df_rsa[\"method\"].unique()[0]\n",
    "\n",
    "# find the distance metric that was used\n",
    "assert (\n",
    "    df_rsa[\"distance_metric\"].nunique() == 1\n",
    "), f\"more than one distance_metric detected: {df_rsa['distance_metric']}\"\n",
    "distance_metric = df_rsa[\"distance_metric\"].unique()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rsa_model_orth = df_rsa[(df_rsa[\"orth\"] == orth) & (df_rsa[\"model\"] == modelname)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:37:38.569340Z",
     "start_time": "2020-09-02T08:37:22.817749Z"
    }
   },
   "outputs": [],
   "source": [
    "with sns.plotting_context(\"talk\"):\n",
    "\n",
    "    # Plot this group\n",
    "    fig, ax = plt.subplots(sharex=True, sharey=True, figsize=(8, 8))\n",
    "\n",
    "    sns.lineplot(\n",
    "        x=\"time_s\",\n",
    "        y=\"similarity\",\n",
    "        ci=68,\n",
    "        hue=\"stopping\",\n",
    "        style=\"sampling\",\n",
    "        hue_order=[\"fixed\", \"variable\"],\n",
    "        style_order=[\"active\", \"yoked\"],\n",
    "        legend=False,\n",
    "        data=df_rsa_model_orth,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    ax.set_title(model)\n",
    "    ax.axhline(0, color=\"black\")\n",
    "    ax.axvline(0.0, color=\"black\", linestyle=\"--\")\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(f\"similarity ({rsa_method})\")\n",
    "\n",
    "    # add legend\n",
    "    # https://matplotlib.org/3.1.1/gallery/text_labels_and_annotations/custom_legends.html\n",
    "    legend_elements = [\n",
    "        Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            color=sns.color_palette()[1],\n",
    "            linestyle=\"-\",\n",
    "            label=\"active/variable\",\n",
    "        ),\n",
    "        Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            color=sns.color_palette()[1],\n",
    "            linestyle=\"--\",\n",
    "            label=\"yoked/variable\",\n",
    "        ),\n",
    "        Line2D(\n",
    "            [0], [0], color=sns.color_palette()[0], linestyle=\"-\", label=\"active/fixed\"\n",
    "        ),\n",
    "        Line2D(\n",
    "            [0], [0], color=sns.color_palette()[0], linestyle=\"--\", label=\"yoked/fixed\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    legend = plt.legend(\n",
    "        handles=legend_elements,\n",
    "        loc=\"best\",\n",
    "        prop={\"size\": 10},\n",
    "        framealpha=1,\n",
    "    )\n",
    "    ax.add_artist(legend)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fname = op.join(outfolder, \"effect_plot.pdf\")\n",
    "    fig.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find significant clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:37:49.547958Z",
     "start_time": "2020-09-02T08:37:38.570529Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the observed clusters in the data\n",
    "clusters_obs, models_obs = return_observed_clusters(\n",
    "    df_rsa_model_orth, thresh, pingouin.mixed_anova\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:37:49.555475Z",
     "start_time": "2020-09-02T08:37:49.549483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load cluster distribution array\n",
    "cluster_distr = np.load(cluster_distr_file)\n",
    "\n",
    "# to turn into data frame, get order of the columns in the array\n",
    "if len(cluster_distr_col_order_file) > 0:\n",
    "    column_orders = []\n",
    "    for thisfile in cluster_distr_col_order_file:\n",
    "        with open(thisfile, \"r\") as fin:\n",
    "            lines = fin.readlines()\n",
    "        column_orders.append([line.strip() for line in lines])\n",
    "\n",
    "    column_order = column_orders[0]\n",
    "    # all read in column order must be the same\n",
    "    assert all([column_order == i for i in column_orders])\n",
    "\n",
    "else:\n",
    "    # NOTE: If we fail to get the column order from a file,\n",
    "    #       we assume it's the same as usual. this is\n",
    "    #       probably safe, and we can double check the results\n",
    "    effect_order = [\"stopping\", \"sampling\", \"Interaction\"]\n",
    "    column_order = [i.lower() for i in effect_order]\n",
    "\n",
    "cluster_distr = pd.DataFrame(cluster_distr, columns=column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:37:49.563354Z",
     "start_time": "2020-09-02T08:37:49.557142Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the significance of the observed clusters\n",
    "(clustersig_threshs, clusters_obs_stats, clusters_obs_sig,) = evaluate_significance(\n",
    "    cluster_distr, clusters_obs, clusterstat, clusterthresh, models_obs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and save p values to file\n",
    "# p value is number of test stats bigger or as big as observed stat, ...\n",
    "# divided by number of permutations\n",
    "pval_lines = []\n",
    "for effect in clusters_obs_stats:\n",
    "\n",
    "    effect_distr = cluster_distr[effect].to_numpy()\n",
    "\n",
    "    for i, stat in enumerate(clusters_obs_stats[effect]):\n",
    "\n",
    "        pval = (1 + np.sum(effect_distr >= stat)) / (1 + len(effect_distr))\n",
    "        pval_lines.append(\n",
    "            f\"effect: {effect}, observed cluster {i+1} (ascending), p={pval}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "# write pvalue as txt\n",
    "pval_fname = cluster_distr_file.replace(\"distr.npy\", \"pvals.txt\")\n",
    "with open(pval_fname, \"w\") as fout:\n",
    "    fout.write(\"\".join(pval_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot significant clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:37:49.801019Z",
     "start_time": "2020-09-02T08:37:49.564769Z"
    }
   },
   "outputs": [],
   "source": [
    "patches = []\n",
    "for effect, clusters in clusters_obs_sig.items():\n",
    "    if len(clusters) < 1:\n",
    "        significant = []\n",
    "        continue  # nothing to do if no significant clusters\n",
    "    elif len(clusters) == 1:\n",
    "        significant = clusters[0]\n",
    "    else:\n",
    "        _summary = \"\\n\".join(\n",
    "            [f\"{len(iclu)}: from {min(iclu)} to {max(iclu)} (idx)\" for iclu in clusters]\n",
    "        )\n",
    "        print(\n",
    "            f\"found several clusters for {effect}, see by length:\\n{_summary}\\n...plotting all.\"\n",
    "        )\n",
    "        significant = np.concatenate(clusters)\n",
    "\n",
    "    xs = df_rsa_model_orth[\"time_s\"].unique()\n",
    "    xs = xs[significant]\n",
    "    ys = np.repeat(ax.get_ylim()[0], xs.shape[-1])\n",
    "    color = dict(zip(column_order, [\"r\", \"g\", \"m\"]))[effect]\n",
    "    (line,) = ax.plot(xs, ys, marker=\".\", color=color, linestyle=\"None\")\n",
    "    line.set_label(effect)\n",
    "\n",
    "\n",
    "ax.legend(loc=2, title=f\"p < {clusterthresh}\")\n",
    "\n",
    "fname = op.join(outfolder, \"effect_perm_plot.pdf\")\n",
    "fig.savefig(fname)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find significant \"average\" effect\n",
    "\n",
    "that is, average the effect over the 4 sampling conditions, and perform a 1-sample ttest permutation test for siginicant clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the kind of model we want\n",
    "average_effect_model = modelname  # e.g., \"numberline\"\n",
    "average_effect_orth = orth  # e.g., False\n",
    "average_effect_modelname = (\n",
    "    \"o\" + average_effect_model if average_effect_orth else average_effect_model\n",
    ")\n",
    "\n",
    "df_modelorth = df_rsa[\n",
    "    (df_rsa[\"model\"] == average_effect_model) & (df_rsa[\"orth\"] == average_effect_orth)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take mean over tasks for each subject\n",
    "df_modelorthmean = df_modelorth.groupby([\"subject\", \"itime\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to matrix\n",
    "nsubjs = df_modelorthmean[\"subject\"].nunique()\n",
    "ntimes = df_modelorthmean[\"itime\"].nunique()\n",
    "X = np.full((nsubjs, ntimes), np.nan)\n",
    "for meta, grp in df_modelorthmean.groupby([\"subject\", \"itime\"]):\n",
    "    subj, itime = meta\n",
    "    isubj = subj - 1\n",
    "    X[isubj, itime] = grp[\"similarity\"].to_numpy()\n",
    "\n",
    "# sanity check df to matrix conversion\n",
    "for i in range(100):\n",
    "    testtime = np.random.choice(range(ntimes))\n",
    "    testsubj = np.random.choice(range(nsubjs))\n",
    "    testsimilarity = X[testsubj, testtime]\n",
    "\n",
    "    tmp = df_modelorthmean[\n",
    "        (df_modelorthmean[\"subject\"] == testsubj + 1)\n",
    "        & (df_modelorthmean[\"itime\"] == testtime)\n",
    "    ][\"similarity\"].to_numpy()[0]\n",
    "\n",
    "    np.testing.assert_allclose(testsimilarity, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats_per_timepoint(X, y, rng):\n",
    "    \"\"\"Calculate one-sample t-test statistic per timepoint in x against y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape(subjects, timepoints)\n",
    "        The similarities per subject and timepoint.\n",
    "    y : float\n",
    "        The value against which to perform a one-sample t-test.\n",
    "    rng : np.random.RandomState | None\n",
    "        A stable rng object for seeding. If an rng object is passed, the\n",
    "        signs in X will be randomly flipped (permuted). If None is passed,\n",
    "        the data are not permuted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats : ndarray, shape(2, timepoints)\n",
    "        The T values on dim 0,t and p values of dim 1,t for each timepoint t.\n",
    "\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    nsubjs, ntimes = X.shape\n",
    "    if rng is not None:\n",
    "        flip = rng.choice([-1, 1], size=((nsubjs, ntimes)), replace=True)\n",
    "        X *= flip\n",
    "\n",
    "    stats = np.zeros((2, ntimes))\n",
    "    for itime in range(ntimes):\n",
    "        tstat, pval = scipy.stats.ttest_1samp(X[..., itime], y)\n",
    "        stats[0, itime] = tstat\n",
    "        stats[1, itime] = pval\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_null_distr(niterations, X, y, rng, thresh):\n",
    "    \"\"\"Generate a null distribution.\"\"\"\n",
    "    distr = np.zeros(niterations)\n",
    "    for iteration in range(niterations):\n",
    "\n",
    "        # Calculate stats for this iteration\n",
    "        stats = calc_stats_per_timepoint(X, y, rng)\n",
    "\n",
    "        # get clusters for this iteration\n",
    "        clusters = _return_clusters(stats[1, ...] < thresh)\n",
    "\n",
    "        # Save max cluster statistic\n",
    "        # use length ... if no clusters found, maximum length is 0\n",
    "        maxstat = 0\n",
    "        _clu_lengths = [len(cluster) for cluster in clusters]\n",
    "        if len(_clu_lengths) > 0:\n",
    "            maxstat = np.max(_clu_lengths)\n",
    "\n",
    "        distr[iteration] = maxstat\n",
    "\n",
    "    return distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the distribution for the null hypothesis\n",
    "# done in parallel\n",
    "with multiprocessing.Pool(njobs) as pool:\n",
    "    niter = int(np.ceil(niterations / njobs))\n",
    "    rngs = [np.random.RandomState(i) for i in range(njobs)]\n",
    "\n",
    "    inputs = itertools.product([niter], [X], [y], rngs, [cluster_defining_thresh_ave])\n",
    "\n",
    "    results = pool.starmap(gen_null_distr, inputs)\n",
    "\n",
    "distr = np.hstack(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed statistics\n",
    "observed_stats = calc_stats_per_timepoint(X, y, rng=None)\n",
    "observed_clusters = _return_clusters(observed_stats[1, ...] < cluster_defining_thresh_ave)\n",
    "\n",
    "# evaluate pvalue of observed clusters against null distribution\n",
    "pvals = [\n",
    "    (1 + np.sum(distr >= len(cluster))) / (1 + len(distr))\n",
    "    for cluster in observed_clusters\n",
    "]\n",
    "\n",
    "observed_clusters_sig = [\n",
    "    clu for clu, p in zip(observed_clusters, pvals) if p < clusterthresh\n",
    "]\n",
    "\n",
    "# save clusters for later plotting\n",
    "general_average_effect_clusters = {}\n",
    "for i, (clu, p) in enumerate(zip(observed_clusters, pvals)):\n",
    "\n",
    "    # compute stats (tval, dof, pval, cohens d)\n",
    "    sel = df_modelorthmean[df_modelorthmean[\"itime\"].isin(clu)]\n",
    "    sel = sel[[\"subject\", \"similarity\", \"itime\"]]\n",
    "    sel = sel.groupby([\"subject\"]).mean().reset_index()\n",
    "    X = sel[\"similarity\"].to_numpy()\n",
    "    assert len(X) == 40\n",
    "    ttest_results = pingouin.ttest(x=X, y=0)\n",
    "    ttest_stats = dict(\n",
    "        tval=float(ttest_results[\"T\"][0]),\n",
    "        dof=float(ttest_results[\"dof\"][0]),\n",
    "        pval=float(ttest_results[\"p-val\"][0]),\n",
    "        cohend=float(ttest_results[\"cohen-d\"][0]),\n",
    "    )\n",
    "\n",
    "    # add the data\n",
    "    tstart = df_rsa_model_orth[\"time_s\"].unique()[clu[0]]\n",
    "    tstop = df_rsa_model_orth[\"time_s\"].unique()[clu[-1]]\n",
    "    general_average_effect_clusters[i] = {\n",
    "        \"p-val\": p,\n",
    "        \"cluster\": clu,\n",
    "        \"tstart\": tstart,\n",
    "        \"tstop\": tstop,\n",
    "        \"ttest-stats\": ttest_stats,\n",
    "    }\n",
    "\n",
    "fname = op.join(outfolder, f\"average_{average_effect_modelname}_clusters.json\")\n",
    "with open(fname, \"w\") as fout:\n",
    "    json.dump(general_average_effect_clusters, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add significance lines to plot as well\n",
    "patches = []\n",
    "average_effect = f\"average_{average_effect_modelname}\"\n",
    "for effect, clusters in {average_effect: observed_clusters_sig}.items():\n",
    "    if len(clusters) < 1:\n",
    "        significant = []\n",
    "        continue  # nothing to do if no significant clusters\n",
    "    elif len(clusters) == 1:\n",
    "        significant = clusters[0]\n",
    "    else:\n",
    "        _summary = \"\\n\".join(\n",
    "            [f\"{len(iclu)}: from {min(iclu)} to {max(iclu)} (idx)\" for iclu in clusters]\n",
    "        )\n",
    "        print(\n",
    "            f\"found several clusters for {effect}, see by length:\\n{_summary}\\n...plotting all.\"\n",
    "        )\n",
    "        significant = np.concatenate(clusters)\n",
    "\n",
    "    xs = df_rsa_model_orth[\"time_s\"].unique()\n",
    "    xs = xs[significant]\n",
    "    ys = np.repeat(ax.get_ylim()[0], xs.shape[-1])\n",
    "    color = \"y\"\n",
    "    (line,) = ax.plot(xs, ys, marker=\".\", color=color, linestyle=\"None\")\n",
    "    line.set_label(effect)\n",
    "\n",
    "\n",
    "ax.legend(loc=2, title=f\"p < {clusterthresh}\")\n",
    "\n",
    "fname = op.join(outfolder, f\"effect_perm_plot_plus_{average_effect_modelname}.pdf\")\n",
    "fig.savefig(fname)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot RDMs over averaged significant cluster windows\n",
    "\n",
    "- for the significant observed clusters\n",
    "    - per subject\n",
    "        - per task\n",
    "        - averaged over tasks\n",
    "    - averaged over subjects\n",
    "        - per task\n",
    "        - averaged over tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_to_plot(rdm):\n",
    "    \"\"\"Remove upper triangle from rdm.\"\"\"\n",
    "    tri_idx = np.triu_indices(rdm.shape[0])\n",
    "    tmprdm = rdm.copy()\n",
    "    tmprdm[tri_idx] = np.nan\n",
    "    return tmprdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T10:01:28.906381Z",
     "start_time": "2020-10-15T10:01:28.897295Z"
    }
   },
   "outputs": [],
   "source": [
    "subjects = range(1, 41)\n",
    "tasks = (\"AF\", \"AV\", \"YF\", \"YV\")\n",
    "\n",
    "rdms_folder_template = op.join(\n",
    "    rsa_folder, \"single_subj_plots\", \"sub-{:02}_task-{}_rdm_times.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T10:22:27.575825Z",
     "start_time": "2020-10-15T10:22:27.558380Z"
    }
   },
   "outputs": [],
   "source": [
    "rdm_plotting_dict = dict(clusters_obs_sig)\n",
    "rdm_plotting_dict.update({average_effect: observed_clusters_sig})\n",
    "\n",
    "# go over effects\n",
    "for effect, clusters in rdm_plotting_dict.items():\n",
    "    if len(clusters) < 1:\n",
    "        significant = []\n",
    "        continue  # nothing to do if no significant clusters\n",
    "    elif len(clusters) == 1:\n",
    "        significant = clusters[0]\n",
    "    else:\n",
    "        _summary = \"\\n\".join(\n",
    "            [f\"{len(iclu)}: from {min(iclu)} to {max(iclu)} (idx)\" for iclu in clusters]\n",
    "        )\n",
    "        print(\n",
    "            f\"found several clusters for {effect}, see by length:\\n{_summary}\\n...picking the largest.\"\n",
    "        )\n",
    "        largest_cluster_idx = np.argmax([len(iclu) for iclu in clusters])\n",
    "        significant = clusters[largest_cluster_idx]\n",
    "\n",
    "    # Go over subjects and tasks\n",
    "    # save RDMs in a large array for later averaging\n",
    "    all_rdms = np.full((len(subjects), len(tasks), 9, 9), np.nan)\n",
    "    did_not_find = 0\n",
    "    for isubj, subj in enumerate(subjects):\n",
    "\n",
    "        for itask, task in enumerate(tasks):\n",
    "\n",
    "            fname = rdms_folder_template.format(subj, task)\n",
    "            if not op.exists(fname):\n",
    "                did_not_find += 1\n",
    "                continue\n",
    "\n",
    "            rdm_times = np.load(fname)\n",
    "            rdm_average = np.mean(rdm_times[..., significant], axis=-1)\n",
    "            all_rdms[isubj, itask, ...] = rdm_average\n",
    "\n",
    "    if did_not_find == len(subjects) * 2:\n",
    "        pass  # everything as expected, 2 of 4 tasks for each subj should be skipped\n",
    "    elif did_not_find == len(subjects) * len(tasks):\n",
    "        print(f\"\\n plotting RDMs for effect: {effect}\")\n",
    "        print(\n",
    "            \"did not save the rdm_times.npy array in a previous run. \"\n",
    "            \"need to re-run the rsa_analysis with newer version of the script.\"\n",
    "        )\n",
    "        continue\n",
    "    else:\n",
    "        raise ValueError(f\"unexpected number of rdm_times.npy found: {did_not_find}\")\n",
    "\n",
    "    # all_rdms is (40 x 4 x 9 x 9) containing the RDMs averaged over the\n",
    "    # significant cluster window ... half of the arrays are NaN, because each\n",
    "    # of the 40 subjs only had 2, instead of 4 tasks\n",
    "\n",
    "    # Make RDM over all subjects\n",
    "    # 2x2 plot (4 panels)\n",
    "    subj_mean = np.nanmean(all_rdms, axis=0)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10, 10))\n",
    "    for itask, task in enumerate(tasks):\n",
    "        ax = axs.flat[itask]\n",
    "        img = ax.imshow(prep_to_plot(subj_mean[itask, ...]))\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        plt.colorbar(img, cax=cax, label=distance_metric)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(task)\n",
    "\n",
    "    fig.suptitle(\"Mean over subjects\", y=1.02)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fname = op.join(outfolder_rdms, effect, \"average_subj_2x2.pdf\")\n",
    "    os.makedirs(op.join(outfolder_rdms, effect), exist_ok=True)\n",
    "    fig.savefig(fname)\n",
    "\n",
    "    # Make RDM over all subjects and tasks\n",
    "    # 1x1 plot (1 panel)\n",
    "    subj_task_mean = np.nanmean(subj_mean, axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    img = ax.imshow(prep_to_plot(subj_task_mean))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(img, cax=cax, label=distance_metric)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Mean over subjects and tasks\")\n",
    "\n",
    "    fname = op.join(outfolder_rdms, effect, \"average_subj_task_1x1.pdf\")\n",
    "    fig.savefig(fname)\n",
    "\n",
    "    # Make RDM for each subject over tasks\n",
    "    # 8x5 plot (40 panels)\n",
    "    task_mean = np.nanmean(all_rdms, axis=1)\n",
    "\n",
    "    fig, axs = plt.subplots(8, 5, sharex=True, sharey=True, figsize=(10, 10))\n",
    "    for isubj, subj in enumerate(subjects):\n",
    "        ax = axs.flat[isubj]\n",
    "        img = ax.imshow(prep_to_plot(task_mean[isubj, ...]))\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        plt.colorbar(img, cax=cax, label=distance_metric)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(subj)\n",
    "\n",
    "    fig.suptitle(\"Mean over tasks\", y=1.01)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fname = op.join(outfolder_rdms, effect, \"average_task_8x5.pdf\")\n",
    "    fig.savefig(fname)\n",
    "\n",
    "    # Make 4 RDMs for each subject and task\n",
    "    # 4 5x4 plots (20 panels per plot)\n",
    "    for itask, task in enumerate(tasks):\n",
    "        rdms = all_rdms[:, itask, ...]\n",
    "\n",
    "        # remove the nan rdms\n",
    "        to_remove = []\n",
    "        for irdm, rdm in enumerate(rdms):\n",
    "            if np.all(np.isnan(rdm)):\n",
    "                to_remove.append(irdm)\n",
    "\n",
    "        rdms = rdms[~np.array(to_remove), ...]\n",
    "\n",
    "        if \"F\" in task:\n",
    "            subj_label = list(range(1, 41, 2))\n",
    "        elif \"V\" in task:\n",
    "            subj_label = list(range(2, 41, 2))\n",
    "\n",
    "        fig, axs = plt.subplots(5, 4, sharex=True, sharey=True, figsize=(10, 10))\n",
    "        for isubj_representation in range(20):\n",
    "            ax = axs.flat[isubj_representation]\n",
    "            img = ax.imshow(prep_to_plot(rdms[isubj_representation, ...]))\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            plt.colorbar(img, cax=cax, label=distance_metric)\n",
    "            ax.set_title(subj_label[isubj_representation])\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        fig.suptitle(f\"task: {task}\", y=1.01)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        fname = op.join(outfolder_rdms, effect, f\"task-{task}_5x4.pdf\")\n",
    "        fig.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2x2 \"summaries\" (plots + stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:37:49.882621Z",
     "start_time": "2020-09-02T08:37:49.802390Z"
    }
   },
   "outputs": [],
   "source": [
    "two_x_two_dict = dict(clusters_obs_sig)\n",
    "two_x_two_dict.update({average_effect: observed_clusters_sig})\n",
    "\n",
    "split_data = {}\n",
    "for effect, clusters in two_x_two_dict.items():\n",
    "    if len(clusters) < 1:\n",
    "        continue\n",
    "\n",
    "    split_data[effect] = {}\n",
    "    for icluster, cluster in enumerate(clusters):\n",
    "\n",
    "        # get three kinds of splits of the data in terms of\n",
    "        # how many samples from each trial are included\n",
    "        data = {\n",
    "            \"both\": df_rsa,\n",
    "            \"first_half\": df_rsa_first_half,\n",
    "            \"second_half\": df_rsa_second_half,\n",
    "        }\n",
    "        dfs = {}\n",
    "        for half, tmp in data.items():\n",
    "            grp = tmp[(tmp[\"orth\"] == orth) & (tmp[\"model\"] == modelname)]\n",
    "\n",
    "            # take mean across significant cluster window\n",
    "            time_idxs = grp[\"itime\"].unique()[cluster]\n",
    "\n",
    "            tmp = grp[grp[\"itime\"].isin(time_idxs)]\n",
    "            tmp = (\n",
    "                tmp.groupby([\"subject\", \"stopping\", \"sampling\", \"orth\", \"model\"])\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "            )\n",
    "            tmp = tmp[\n",
    "                [\"subject\", \"stopping\", \"sampling\", \"orth\", \"model\", \"similarity\"]\n",
    "            ]\n",
    "\n",
    "            # tmp \"save\"\n",
    "            dfs[half] = tmp\n",
    "\n",
    "        # save to overall data dict\n",
    "        split_data[effect][icluster] = dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:37:50.678966Z",
     "start_time": "2020-09-02T08:37:49.883845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make plots\n",
    "do_posthocs = []\n",
    "for effect in split_data:\n",
    "    for icluster in split_data[effect]:\n",
    "\n",
    "        # make a figure\n",
    "        fig, axs = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(10, 5))\n",
    "\n",
    "        for isplit, split in enumerate(split_data[effect][icluster]):\n",
    "            df = split_data[effect][icluster][split]\n",
    "            ax = axs.flat[isplit]\n",
    "\n",
    "            combination = f\"{effect}-{icluster}-{split}\"\n",
    "            print(combination)\n",
    "            fname = op.join(outfolder, \"anova_\" + combination + \".html\")\n",
    "            fout = open(fname, \"w\")\n",
    "            print(combination, file=fout)\n",
    "\n",
    "            # stats\n",
    "            model = pingouin.mixed_anova(\n",
    "                data=df,\n",
    "                dv=\"similarity\",\n",
    "                within=\"sampling\",\n",
    "                subject=\"subject\",\n",
    "                between=\"stopping\",\n",
    "            )\n",
    "\n",
    "            print(model.to_html(), file=fout)\n",
    "            fout.close()\n",
    "\n",
    "            if effect in [\"sampling\", \"stopping\", \"interaction\"]:\n",
    "                pval = model[model[\"Source\"].str.lower() == effect][\"p-unc\"].to_numpy()[\n",
    "                    0\n",
    "                ]\n",
    "\n",
    "                if pval < clusterthresh:\n",
    "                    do_posthocs.append((effect, icluster, split))\n",
    "            else:\n",
    "                # we are dealing with an \"average effect\", so we do a posthoc\n",
    "                # test no matter what the p values are for sampling, stopping,\n",
    "                # and interaction\n",
    "                do_posthocs.append((effect, icluster, split))\n",
    "\n",
    "            # special treatment for AV YV and AF YF comparisons\n",
    "            # this treatment is not needed if we deal with an \"average effect\"\n",
    "            if effect in [\"sampling\", \"stopping\", \"interaction\"]:\n",
    "                ttest_pvals = {}\n",
    "                for _stopping in [\"variable\", \"fixed\"]:\n",
    "                    x = df[\n",
    "                        (df[\"sampling\"] == \"active\") & (df[\"stopping\"] == _stopping)\n",
    "                    ][\"similarity\"].to_numpy()\n",
    "                    y = df[(df[\"sampling\"] == \"yoked\") & (df[\"stopping\"] == _stopping)][\n",
    "                        \"similarity\"\n",
    "                    ].to_numpy()\n",
    "                    ttest_results = pingouin.ttest(x, y, paired=True)\n",
    "                    ttest_pvals[_stopping] = ttest_results[\"p-val\"][0]\n",
    "\n",
    "            # plot\n",
    "            sns.pointplot(\n",
    "                x=\"sampling\",\n",
    "                y=\"similarity\",\n",
    "                hue=\"stopping\",\n",
    "                hue_order=[\"fixed\", \"variable\"],\n",
    "                data=df,\n",
    "                ci=68,\n",
    "                dodge=True,\n",
    "                ax=ax,\n",
    "            )\n",
    "\n",
    "            # plot a \"zero\" line\n",
    "            ax.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "            if effect in [\"sampling\", \"stopping\", \"interaction\"]:\n",
    "                title = f\"{split}, p={pval:.3f}\\n\"\n",
    "                for _stop, pval in ttest_pvals.items():\n",
    "                    title += (\n",
    "                        f\"\\nA{_stop[0].upper()} vs Y{_stop[0].upper()}: p={pval:.3f}\"\n",
    "                    )\n",
    "            else:\n",
    "                # we are dealing with an \"average effect\", so we report\n",
    "                # all \"effects\" (sampling, stopping, interaction)\n",
    "                title = f\"{split}\"\n",
    "                for _src, _pval in zip(\n",
    "                    model[\"Source\"].to_list(), model[\"p-unc\"].to_list()\n",
    "                ):\n",
    "                    title += f\"\\n{_src}: p={_pval:.3f}\"\n",
    "\n",
    "            ax.set_title(title)\n",
    "\n",
    "            if isplit != 0:\n",
    "                ax.get_legend().remove()\n",
    "\n",
    "        fig.suptitle(f\"{effect}: cluster {icluster}\", y=1.1)\n",
    "\n",
    "        fname = op.join(outfolder, f\"2x2_{effect}_{icluster}.pdf\")\n",
    "        fig.savefig(fname, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T09:07:24.750822Z",
     "start_time": "2020-09-02T09:07:24.571773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make posthoc comparisons for significant 2x2 summaries\n",
    "# do for each \"half split\" ... disregarding significance\n",
    "for effect, icluster, _ in do_posthocs:\n",
    "    for split in (\"both\", \"first_half\", \"second_half\"):\n",
    "        df = split_data[effect][icluster][split]\n",
    "\n",
    "        combination = f\"{effect}-{icluster}-{split}\"\n",
    "        print(combination)\n",
    "        fname = op.join(outfolder, \"posthocs_\" + combination + \".html\")\n",
    "        fout = open(fname, \"w\")\n",
    "        print(combination, file=fout)\n",
    "\n",
    "        for within_first in (True, False):\n",
    "\n",
    "            stat = pingouin.pairwise_ttests(\n",
    "                data=df,\n",
    "                dv=\"similarity\",\n",
    "                within=\"sampling\",\n",
    "                subject=\"subject\",\n",
    "                between=\"stopping\",\n",
    "                within_first=within_first,\n",
    "                padjust=\"bonf\",\n",
    "                effsize=\"cohen\",\n",
    "                return_desc=True,\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                display(stat)\n",
    "            except NameError:\n",
    "                print(stat)\n",
    "            print(stat.to_html(), file=fout)\n",
    "\n",
    "        fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation with behavioral accuracy and BNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T13:08:18.784641Z",
     "start_time": "2020-12-01T13:08:18.724640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load behavioral accuracy data\n",
    "fname_beh_acc = \"beh_accs.csv\"\n",
    "accuracy_df = pd.read_csv(fname_beh_acc)\n",
    "\n",
    "# also load BNT data, as part of the overall behavioral DF\n",
    "fname_beh_overall = \"behavioral_data.csv\"\n",
    "beh_df = pd.read_csv(fname_beh_overall)\n",
    "\n",
    "# also load CPP accumulation data\n",
    "fname_accumul = op.join(BIDS_ROOT, \"code\", \"publication_plots\", \"eeg_accumulation.csv\")\n",
    "accumul_df = pd.read_csv(fname_accumul)\n",
    "accumul_colname = \"mean_amp_diff_late_minus_early\"\n",
    "assert accumul_df.columns[-1] == accumul_colname\n",
    "accumul_df = accumul_df.rename(columns={accumul_colname: \"cpp_accumulation\"})\n",
    "\n",
    "# Drop DESC task for now\n",
    "accuracy_df = accuracy_df[accuracy_df[\"task\"] != \"DESC\"].reset_index(drop=True)\n",
    "beh_df = beh_df[beh_df[\"task\"] != \"DESC\"].reset_index(drop=True)\n",
    "\n",
    "# get sampling and stopping from task\n",
    "accuracy_df[\"sampling\"] = accuracy_df[\"task\"].str[0].map({\"A\": \"active\", \"Y\": \"yoked\"})\n",
    "accuracy_df[\"stopping\"] = (\n",
    "    accuracy_df[\"task\"].str[1].map({\"F\": \"fixed\", \"V\": \"variable\"})\n",
    ")\n",
    "\n",
    "beh_df[\"sampling\"] = beh_df[\"task\"].str[0].map({\"A\": \"active\", \"Y\": \"yoked\"})\n",
    "beh_df[\"stopping\"] = beh_df[\"task\"].str[1].map({\"F\": \"fixed\", \"V\": \"variable\"})\n",
    "\n",
    "accumul_df[\"sampling\"] = accumul_df[\"sampling\"].str.lower()\n",
    "accumul_df[\"stopping\"] = accumul_df[\"stopping\"].str.lower()\n",
    "\n",
    "# work on \"experienced\" accuracy\n",
    "accuracy_type = \"experienced\"\n",
    "accuracy_df = accuracy_df[accuracy_df[\"accuracy_type\"] == accuracy_type]\n",
    "\n",
    "# work on behavioral df to make it focussed on bnt only\n",
    "bnt_df = beh_df[[\"subject\", \"stopping\", \"sampling\", \"bnt_quartile\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:37:50.889250Z",
     "start_time": "2020-09-02T08:37:50.886481Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_methods = {\n",
    "    \"correct_choice\": \"pearson\",\n",
    "    \"bnt_quartile\": \"kendall\",\n",
    "    \"cpp_accumulation\": \"pearson\",\n",
    "}\n",
    "tail = \"two-sided\"  # expecting a positive coefficient: better encoding = higher accuracy or BNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:37:51.029665Z",
     "start_time": "2020-09-02T08:37:50.890682Z"
    }
   },
   "outputs": [],
   "source": [
    "info = f\"tail -> {tail}\\naccuracy -> {accuracy_type}\\n\"\n",
    "print(info)\n",
    "\n",
    "for effect, icluster, split in do_posthocs:\n",
    "\n",
    "    combination = f\"{effect}-{icluster}-{split}\"\n",
    "    print(combination)\n",
    "    fname = op.join(outfolder, \"corrs_\" + combination + \".html\")\n",
    "    fout = open(fname, \"w\")\n",
    "    print(combination, file=fout)\n",
    "    print(\"<br>\" + info, file=fout)\n",
    "\n",
    "    df = split_data[effect][icluster][split]\n",
    "\n",
    "    # Merge accuracy, BNT, and CPP-accumulation data onto current df\n",
    "    tmp = df.merge(accuracy_df, on=[\"subject\", \"stopping\", \"sampling\"])\n",
    "    tmp = tmp.merge(bnt_df, on=[\"subject\", \"stopping\", \"sampling\"])\n",
    "    tmp = tmp.merge(accumul_df, on=[\"subject\", \"stopping\", \"sampling\"])\n",
    "\n",
    "    # Calculate correlations for each sampling condition\n",
    "    for meta, grp in tmp.groupby([\"task\"]):\n",
    "        print(meta)\n",
    "        print(\"<br>\" + meta, file=fout)\n",
    "\n",
    "        y = \"correct_choice\"\n",
    "        stat_acc = pingouin.correlation.corr(\n",
    "            grp[\"similarity\"], grp[y], method=corr_methods[y], tail=tail\n",
    "        )\n",
    "\n",
    "        y = \"bnt_quartile\"\n",
    "        stat_bnt = pingouin.correlation.corr(\n",
    "            grp[\"similarity\"], grp[y], method=corr_methods[y], tail=tail\n",
    "        )\n",
    "\n",
    "        y = \"cpp_accumulation\"\n",
    "        stat_cpp = pingouin.correlation.corr(\n",
    "            grp[\"similarity\"], grp[y], method=corr_methods[y], tail=tail\n",
    "        )\n",
    "\n",
    "        tmp_print_info = False\n",
    "        try:\n",
    "            display(stat_acc)\n",
    "            display(stat_bnt)\n",
    "            display(stat_cpp)\n",
    "        except NameError:\n",
    "            tmp_print_info = True\n",
    "\n",
    "        for y, stat in {\n",
    "            \"correct_choice\": stat_acc,\n",
    "            \"bnt_quartile\": stat_bnt,\n",
    "            \"cpp_accumulation\": stat_cpp,\n",
    "        }.items():\n",
    "            print(f\"<br> {y}\", file=fout)\n",
    "            print(stat.to_html(), file=fout)\n",
    "            if tmp_print_info:\n",
    "                print(f\"{y}\\n\", stat)\n",
    "\n",
    "    fout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "374.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
