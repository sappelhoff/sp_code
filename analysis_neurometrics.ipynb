{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T15:09:42.914227Z",
     "start_time": "2021-05-30T15:09:42.620185Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T15:09:45.915076Z",
     "start_time": "2021-05-30T15:09:43.087424Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "from numba import njit\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from utils import (\n",
    "    BIDS_ROOT,\n",
    "    _kendall_tau_a,\n",
    "    extract_sample_frequencies,\n",
    "    find_time_idxs,\n",
    "    get_erps_by_dict,\n",
    "    prep_epochs,\n",
    "    spm_orth,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T15:09:45.920707Z",
     "start_time": "2021-05-30T15:09:45.917074Z"
    }
   },
   "outputs": [],
   "source": [
    "# IO: Where to find the data\n",
    "# Where to find epochs\n",
    "fname_epo_template = os.path.join(\n",
    "    BIDS_ROOT, \"derivatives\", \"sub-{0:02}\", \"sub-{0:02}_epochs-epo.fif.gz\"\n",
    ")\n",
    "\n",
    "\n",
    "# Pack all names in a dict\n",
    "name_templates = dict()\n",
    "name_templates[\"epochs\"] = fname_epo_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T15:09:45.935872Z",
     "start_time": "2021-05-30T15:09:45.922883Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing settings\n",
    "preproc_settings = {\n",
    "    \"crop\": (0.6, 1.6),\n",
    "    \"tshift\": -0.8,\n",
    "    \"smooth\": 250,\n",
    "    \"baseline\": (None, 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T15:09:51.337830Z",
     "start_time": "2021-05-30T15:09:45.938281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get an ERP for extracting times array\n",
    "# it's the same for all ERPs and will be used below\n",
    "tmp = get_erps_by_dict({\"all\": list()}, name_templates, [1], **preproc_settings)\n",
    "times = tmp[\"all\"][0].times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurometrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## settings and func defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T15:09:54.688558Z",
     "start_time": "2021-05-30T15:09:54.681616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Where the RSA results are stored\n",
    "results_folder = \"cv-False_flip-False_rsa-pearson_dist-euclidean_half-both_\"\n",
    "results_folder += \"exclude-Identity_mnn-FalseFalseFalse_ec-False_\"\n",
    "results_folder += \"c-(0.6, 1.6)t--0.8s-150b-(None, 0)s-True_outcome\"\n",
    "\n",
    "rsa_results_path = os.path.join(\n",
    "    BIDS_ROOT,\n",
    "    \"derivatives\",\n",
    "    \"rsa_9x9\",\n",
    "    results_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T15:09:55.436416Z",
     "start_time": "2021-05-30T15:09:55.116279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "# -----\n",
    "# Define constants\n",
    "subjects = np.arange(1, 41)\n",
    "numbers = np.arange(1, 10)\n",
    "tasks = (\"AF\", \"AV\", \"YF\", \"YV\")\n",
    "\n",
    "rdms_folder_template = os.path.join(\n",
    "    rsa_results_path, \"single_subj_plots\", \"sub-{:02}_task-{}_rdm_times.npy\"\n",
    ")\n",
    "\n",
    "# Read behavioral data to get mapping which subj was in which task\n",
    "beh_fname = \"behavioral_data.csv\"\n",
    "beh_fpath = os.path.join(BIDS_ROOT, \"code\", beh_fname)\n",
    "df = pd.read_csv(beh_fpath)\n",
    "\n",
    "# Determine based on what time window in EEG-RDM to calculate correlations\n",
    "custom_window = (0.3, 0.6)\n",
    "max_cluster = find_time_idxs(custom_window, times)\n",
    "\n",
    "# numerical distance\n",
    "# rescale numbers 1 to 9 to range -1, 1\n",
    "numbers_rescaled = np.interp(numbers, (numbers.min(), numbers.max()), (-1, +1))\n",
    "\n",
    "# lower triangle indices for later extracting vector from RDMs\n",
    "lower_triangle_idx = np.tril_indices(numbers.shape[0], k=-1)\n",
    "\n",
    "# method for correlating model-RDM and EEG-RDM\n",
    "corr_method = \"pearson\"\n",
    "do_orth = True\n",
    "orth_demean = True\n",
    "orth_other_model = False  # if True, orth sfreq, num, ext ... else only sfreq num/ext\n",
    "wilcoxon = False\n",
    "relative_to_b0_k1 = True\n",
    "\n",
    "# granularity of bias and kappa parameters\n",
    "n = 300\n",
    "\n",
    "# bias and kappa param values to use\n",
    "_b = 0.75\n",
    "biases = np.linspace(-_b, _b, n)\n",
    "kappas = np.linspace(0.5, 10.0, n)\n",
    "bias_kappa_combis = list(itertools.product(biases, kappas))\n",
    "\n",
    "biases = np.unique(np.array(bias_kappa_combis)[:, 0])\n",
    "kappas = np.unique(np.array(bias_kappa_combis)[:, 1])\n",
    "bias_0_idx = (np.abs(biases - 0)).argmin()\n",
    "kappa_1_idx = (np.abs(kappas - 1)).argmin()\n",
    "\n",
    "# How many jobs to run in parallel\n",
    "NJOBS = multiprocessing.cpu_count() - 1\n",
    "\n",
    "# Whether or not to save plots\n",
    "plotsave = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T13:32:21.305955Z",
     "start_time": "2021-05-02T13:32:21.296084Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    eeg_rdms\n",
    "    assert eeg_rdms.shape == (40, 4, 9, 9)\n",
    "    assert eeg_rdms.flat[1] == 4.553910558275579, \"item in eeg_rdms unexpected\"\n",
    "    print(\"EEG-RDMs already loaded.\")\n",
    "except NameError:\n",
    "    # Read EEG-RDMs\n",
    "    # -------------\n",
    "    # Go over subjects and tasks\n",
    "    # save RDMs in a large array for later averaging\n",
    "    # eeg_rdms is (40 x 4 x 9 x 9) containing the RDMs averaged over the\n",
    "    # max_cluster window ... half of the arrays are NaN, because each\n",
    "    # of the 40 subjs only had 2, instead of 4 tasks\n",
    "    eeg_rdms = np.full((len(subjects), len(tasks), 9, 9), np.nan)\n",
    "    did_not_find = 0\n",
    "    for isubj, subj in enumerate(subjects):\n",
    "\n",
    "        for itask, task in enumerate(tasks):\n",
    "\n",
    "            fname = rdms_folder_template.format(subj, task)\n",
    "            if not os.path.exists(fname):\n",
    "                did_not_find += 1\n",
    "                continue\n",
    "\n",
    "            rdm_times = np.load(fname)\n",
    "            rdm_average = np.mean(rdm_times[..., max_cluster], axis=-1)\n",
    "            eeg_rdms[isubj, itask, ...] = rdm_average\n",
    "\n",
    "    # sanity check: 2 of 4 tasks for each subj should be skipped\n",
    "    if did_not_find != len(subjects) * 2:\n",
    "        raise ValueError(f\"unexpected number of rdm_times.npy found: {did_not_find}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T13:32:21.491417Z",
     "start_time": "2021-05-02T13:32:21.481886Z"
    }
   },
   "outputs": [],
   "source": [
    "# produce model-RDMs (numberline, extremity) based on \"biased and kappaed\" numbers\n",
    "# --------------------------------------------------------------------------------\n",
    "@njit\n",
    "def eq1(X, bias, kappa):\n",
    "    \"\"\"See equation 1 from Spitzer et al. 2017, Nature Human Behavior.\"\"\"\n",
    "    dv = np.sign(X + bias) * (np.abs(X + bias) ** kappa)\n",
    "    return dv\n",
    "\n",
    "\n",
    "def produce_model_rdms(model_to_use, bias_kappa_combis):\n",
    "    \"\"\"Using different combinations of bias and kappa, produce model RDMs.\n",
    "\n",
    "    Scale numbers from 1 to 9 first to a range -1 to 1, and then distort them\n",
    "    based on equation 1 from Spitzer et al. 2017 (NHB), using different\n",
    "    combinations of bias and kappa parameters. The build an RDM from each\n",
    "    resulting set of distorted numbers: For numberline, just take the numbers\n",
    "    as they are - and for extremity, take the absolute value of the numbers\n",
    "    (see Notes).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_to_use : {\"numberline\", \"extremity\"}\n",
    "        Which model RDMs to produce.\n",
    "    bias_kappa_combias : list of tuple of len 2\n",
    "        Each tuple in the list is a unique combination of (bias, kappa).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model_rdms : np.ndarray, shape(9, 9, len(bias_kappa_combis))\n",
    "        The model RDMs for either numberline or extremity.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For extremity, we take the absolute of the distorted values, because it\n",
    "    corresponds to how we calculate extremity on the non-re-scaled,\n",
    "    non-distorted values 1, 2, ..., 9: first center them around zero, and\n",
    "    then take the absolute:\n",
    "\n",
    "    ``np.abs(np.arange(1, 10) - np.median(np.arange(1, 10)))``\n",
    "\n",
    "    In our steps we already centered on zero through rescaling to -1, 1.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    eq1\n",
    "\n",
    "    \"\"\"\n",
    "    numbers = np.arange(1, 10)\n",
    "    numbers_rescaled = np.interp(numbers, (numbers.min(), numbers.max()), (-1.0, +1.0))\n",
    "\n",
    "    model_rdms = np.full(\n",
    "        (\n",
    "            len(numbers),\n",
    "            len(numbers),\n",
    "            len(bias_kappa_combis),\n",
    "        ),\n",
    "        np.nan,\n",
    "    )\n",
    "    for i, (bias, kappa) in enumerate(bias_kappa_combis):\n",
    "\n",
    "        # Calculate DV for each rescaled number given bias and kappa\n",
    "        dv_vector = eq1(numbers_rescaled, bias, kappa)\n",
    "\n",
    "        # If we want to produce extremity RDMs, absolute, see Notes in docstr\n",
    "        if model_to_use == \"extremity\":\n",
    "            dv_vector = np.abs(dv_vector)\n",
    "\n",
    "        else:\n",
    "            assert model_to_use == \"numberline\", \"unknown `model_to_use`.\"\n",
    "\n",
    "        # Calculate RDM based on DVs\n",
    "        arrs = []\n",
    "        for dv in dv_vector:\n",
    "            arrs.append(np.abs(dv_vector - dv))\n",
    "        model_rdms[..., i] = np.stack(arrs, axis=0)\n",
    "\n",
    "    # Sanity check: No NaNs\n",
    "    assert not np.isnan(model_rdms).any()\n",
    "    return model_rdms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T13:32:27.917176Z",
     "start_time": "2021-05-02T13:32:21.610768Z"
    }
   },
   "outputs": [],
   "source": [
    "# model RDMs (constants)\n",
    "NUMBERLINE_RDMS = produce_model_rdms(\"numberline\", bias_kappa_combis)\n",
    "EXTREMITY_RDMS = produce_model_rdms(\"extremity\", bias_kappa_combis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T13:32:27.936889Z",
     "start_time": "2021-05-02T13:32:27.918463Z"
    }
   },
   "outputs": [],
   "source": [
    "def corr_model_and_eeg_rdms(\n",
    "    subject,\n",
    "    itask,\n",
    "    model_rdms,\n",
    "    eeg_rdms,\n",
    "    corr_method,\n",
    "    bias_kappa_combis,\n",
    "    lower_triangle_idx,\n",
    "    df,\n",
    "    do_orth,\n",
    "    orth_demean,\n",
    "    orth_other_model,\n",
    "):\n",
    "    \"\"\"Correlate model-RDMs and EEG-RDMs per task and subject.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subject : int\n",
    "        The subject ID (1 to 40).\n",
    "    itask : int | \"all\"\n",
    "        Index into the task array ``('AF', 'AV', 'YF', 'YV')``. Not that\n",
    "        not all subjects have performed all tasks. This needs to be handled\n",
    "        outside of this function. Only pass subject-task combinations that\n",
    "        do exist in the data. If \"all\", the `eeg_rdms` must be of\n",
    "        shape (40, 9, 9).\n",
    "    model_rdms : np.ndarray, shape(9, 9, n)\n",
    "        All ``n`` model RDMs (9 by 9) that were created based on ``eq1``.\n",
    "    eeg_rdms : np.ndarray, shape(40, 4, 9, 9)\n",
    "        All EEG RDMs (9 by 9), for all subjects (40), and tasks (4). Contains\n",
    "        NaN values for some slices, because not all subjects performed all\n",
    "        tasks. These RDMs are averaged over some time period.\n",
    "    corr_method : str\n",
    "        How to correlate the model and EEG RDMs. Must be one of\n",
    "        [pearson, spearman, kendall_a, kendall_b].\n",
    "    bias_kappa_combis : list of tuple\n",
    "        Each tuple in the list contains (bias, kappa) floats.\n",
    "    lower_triangle_idx : np.ndarray\n",
    "        Indices to extract the lower triangle from a 9 by 9 matrix.\n",
    "    df : pandas.DataFrame\n",
    "        The behavioral data.\n",
    "    do_orth : bool\n",
    "        Whether or not to orthogonalize model.\n",
    "    orth_demean : bool\n",
    "        Whether or not to demean each column before orthogonalization.\n",
    "    orth_other_model : bool\n",
    "        Whether to orthogonalize sfreq, numberline, extremity. If False,\n",
    "        only orthogonalize sfreq with either numberline or extremity.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    subject : int\n",
    "        The subject.\n",
    "    itask : int\n",
    "        The task index into the tuple (\"AF\", \"AV\", \"YF\", \"YV\").\n",
    "    coefs : np.ndarray, shape(n, n)\n",
    "        The correlation coefficients for each bias/kappa combination. The\n",
    "        bias parameters are on the x-axis of the 2D array (increasing left\n",
    "        to right), the kappa parameters are on the y-axis of the 2D array\n",
    "        (increasing top to bottom). See `bias_kappa_combis` for the parameter\n",
    "        values (``biases=np.unique(np.array(bias_kappa_combis)[:, 0])``,\n",
    "        ``kappas=np.unique(np.array(bias_kappa_combis)[:, 1])``)\n",
    "    pvals : np.ndarray, shape(n, n)\n",
    "        The two-tailed p-value corresponding to each correlation coefficient.\n",
    "    \"\"\"\n",
    "    # Get sampling frequency model vector for this subj and task\n",
    "    tasks = (\"AF\", \"AV\", \"YF\", \"YV\")\n",
    "    _df = df[df[\"subject\"] == subject]\n",
    "    if isinstance(itask, str):\n",
    "        # we are working on \"all\" tasks at once\n",
    "        assert itask == \"all\", \"itask must be int, or 'all'.\"\n",
    "        assert eeg_rdms.shape == (40, 9, 9), \"if itask is 'all', pass all eeg_rdms.\"\n",
    "    else:\n",
    "        # we are working on a specific task\n",
    "        # make further sub-selection into task\n",
    "        _df = _df[_df[\"task\"] == tasks[itask]]\n",
    "\n",
    "    sfreqs = extract_sample_frequencies(_df, with_sides=False)\n",
    "\n",
    "    # normalize to be in -1, 1 range\n",
    "    sfreqs = np.interp(sfreqs, (sfreqs.min(), sfreqs.max()), (-1, +1))\n",
    "\n",
    "    # build RDM\n",
    "    arrs = []\n",
    "    for sf in sfreqs:\n",
    "        arrs.append(np.abs(sfreqs - sf))\n",
    "\n",
    "    # extract vector\n",
    "    mod_sfreq = np.stack(arrs, axis=0)[lower_triangle_idx].flatten()\n",
    "    if orth_demean:\n",
    "        mod_sfreq -= mod_sfreq.mean()\n",
    "\n",
    "    # ----------------------------\n",
    "    # prep coef and pval arrays\n",
    "    coefs = np.full(len(bias_kappa_combis), np.nan)\n",
    "    pvals = np.full(len(bias_kappa_combis), np.nan)\n",
    "\n",
    "    # get data (EEG)\n",
    "    if itask == \"all\":\n",
    "        assert eeg_rdms.shape == (40, 9, 9)\n",
    "        eeg_rdm_vector = eeg_rdms[subject - 1, ...]\n",
    "    else:\n",
    "        assert eeg_rdms.shape == (40, 4, 9, 9)\n",
    "        eeg_rdm_vector = eeg_rdms[subject - 1, itask, ...]\n",
    "\n",
    "    eeg_rdm_vector = eeg_rdm_vector[lower_triangle_idx].flatten()\n",
    "\n",
    "    # Get data (model) per bias-kappa combination and correlate with EEG\n",
    "    for icombi in range(len(bias_kappa_combis)):\n",
    "        mod_rdm_vector = model_rdms[..., icombi][lower_triangle_idx].flatten()\n",
    "\n",
    "        # Get the other rdm vector (either numberline or extremity)\n",
    "        mod_other_num = NUMBERLINE_RDMS[..., icombi][lower_triangle_idx].flatten()\n",
    "        mod_other_ext = EXTREMITY_RDMS[..., icombi][lower_triangle_idx].flatten()\n",
    "\n",
    "        if np.array_equal(mod_rdm_vector, mod_other_num):\n",
    "            mod_other = mod_other_ext\n",
    "        elif np.array_equal(mod_rdm_vector, mod_other_ext):\n",
    "            mod_other = mod_other_num\n",
    "        else:\n",
    "            raise ValueError(\"mod_rdm_vector not equivalent to either num or ext.\")\n",
    "\n",
    "        # orthogonalize recursively\n",
    "        if do_orth:\n",
    "            prev_shape = mod_rdm_vector.shape\n",
    "\n",
    "            if orth_demean:\n",
    "                # mean-center first\n",
    "                mod_other -= mod_other.mean()\n",
    "                # mod_sfreq is demeaned out of the loop (see above)\n",
    "                mod_rdm_vector -= mod_rdm_vector.mean()\n",
    "\n",
    "            # last column is orthed.\n",
    "            if orth_other_model:\n",
    "                vecs = [\n",
    "                    mod_other.reshape(-1, 1),\n",
    "                    mod_sfreq.reshape(-1, 1),\n",
    "                    mod_rdm_vector.reshape(-1, 1),\n",
    "                ]\n",
    "            else:\n",
    "                vecs = [\n",
    "                    mod_sfreq.reshape(-1, 1),\n",
    "                    mod_rdm_vector.reshape(-1, 1),\n",
    "                ]\n",
    "            X = np.hstack(vecs)\n",
    "            assert X.shape[-1] == len(vecs), \"columns should be RDM types\"\n",
    "            X_orth = spm_orth(X)\n",
    "\n",
    "            # done with orth:\n",
    "            mod_rdm_vector = X_orth[:, -1].reshape(prev_shape)\n",
    "\n",
    "        if np.all(mod_rdm_vector == mod_rdm_vector[0]):\n",
    "            _task = \"all\" if itask == \"all\" else tasks[itask]\n",
    "            print(\n",
    "                f\"equal at {icombi}, {bias_kappa_combis[icombi]}, {subject}, {_task}: {mod_rdm_vector}\\n\"\n",
    "            )\n",
    "\n",
    "        if corr_method == \"pearson\":\n",
    "            coef, pval = scipy.stats.pearsonr(eeg_rdm_vector, mod_rdm_vector)\n",
    "        elif corr_method == \"spearman\":\n",
    "            coef, pval = scipy.stats.spearmanr(eeg_rdm_vector, mod_rdm_vector)\n",
    "        elif corr_method == \"kendall_a\":\n",
    "            coef = _kendall_tau_a(eeg_rdm_vector, mod_rdm_vector)\n",
    "            # no pvals implemented in _kendall_tau_a\n",
    "            pval = np.nan\n",
    "        elif corr_method == \"kendall_b\":\n",
    "            coef, pval = scipy.stats.kendalltau(eeg_rdm_vector, mod_rdm_vector)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected corr_method: {corr_method}\")\n",
    "\n",
    "        coefs[icombi] = coef\n",
    "        pvals[icombi] = pval\n",
    "\n",
    "    # Reshape into maps and return\n",
    "    n = int(np.sqrt(len(bias_kappa_combis)))\n",
    "    coefs = coefs.reshape(n, n).T\n",
    "    pvals = pvals.reshape(n, n).T\n",
    "\n",
    "    return subject, itask, coefs, pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T13:32:27.963276Z",
     "start_time": "2021-05-02T13:32:27.939246Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_coef_maps(\n",
    "    tasks,\n",
    "    df,\n",
    "    model_rdms,\n",
    "    eeg_rdms,\n",
    "    corr_method,\n",
    "    bias_kappa_combis,\n",
    "    lower_triangle_idx,\n",
    "    do_orth,\n",
    "    orth_demean,\n",
    "    orth_other_model,\n",
    "    NJOBS,\n",
    "    relative_to_b0_k1,\n",
    "    bias_0_idx,\n",
    "    kappa_1_idx,\n",
    "):\n",
    "    \"\"\"Generate coef_map and pval_mamp_subjs.\"\"\"\n",
    "    # Prepare all input combinations for multiprocessing.Pool.starmap\n",
    "    if isinstance(tasks, str):\n",
    "        assert tasks == \"all\"\n",
    "        assert eeg_rdms.shape == (40, 9, 9)\n",
    "        param_combis = []\n",
    "        subjects = df[\"subject\"].unique()\n",
    "        for subject in subjects:\n",
    "            params = dict(\n",
    "                subject=subject,\n",
    "                itask=\"all\",\n",
    "                model_rdms=model_rdms,\n",
    "                eeg_rdms=eeg_rdms,\n",
    "                corr_method=corr_method,\n",
    "                bias_kappa_combis=bias_kappa_combis,\n",
    "                lower_triangle_idx=lower_triangle_idx,\n",
    "                df=df,\n",
    "                do_orth=do_orth,\n",
    "                orth_demean=orth_demean,\n",
    "                orth_other_model=orth_other_model,\n",
    "            )\n",
    "            param_combis.append(list(params.values()))\n",
    "    else:\n",
    "        assert eeg_rdms.shape == (40, 4, 9, 9)\n",
    "        param_combis = []\n",
    "        for itask, task in enumerate(tasks):\n",
    "            subjects = df[df[\"task\"] == task][\"subject\"].unique()\n",
    "            for subject in subjects:\n",
    "                params = dict(\n",
    "                    subject=subject,\n",
    "                    itask=itask,\n",
    "                    model_rdms=model_rdms,\n",
    "                    eeg_rdms=eeg_rdms,\n",
    "                    corr_method=corr_method,\n",
    "                    bias_kappa_combis=bias_kappa_combis,\n",
    "                    lower_triangle_idx=lower_triangle_idx,\n",
    "                    df=df,\n",
    "                    do_orth=do_orth,\n",
    "                    orth_demean=orth_demean,\n",
    "                    orth_other_model=orth_other_model,\n",
    "                )\n",
    "                param_combis.append(list(params.values()))\n",
    "\n",
    "    # Process inputs in parallel\n",
    "    with multiprocessing.Pool(NJOBS) as pool:\n",
    "        results = pool.starmap(corr_model_and_eeg_rdms, param_combis)\n",
    "\n",
    "    # store data in array (subj, task, kappas, biases)\n",
    "    # will contain NaN for some subj-task combinations\n",
    "    if tasks == \"all\":\n",
    "        coef_map = np.full((40, n, n), np.nan)\n",
    "        pval_map_subjs = np.full((40, n, n), np.nan)\n",
    "    else:\n",
    "        coef_map = np.full((40, 4, n, n), np.nan)\n",
    "        pval_map_subjs = np.full((40, 4, n, n), np.nan)\n",
    "    for res in results:\n",
    "        subject, itask, coefs, pvals = res\n",
    "        if itask == \"all\":\n",
    "            coef_map[subject - 1, ...] = coefs\n",
    "            pval_map_subjs[subject - 1, ...] = pvals\n",
    "        else:\n",
    "            coef_map[subject - 1, itask, ...] = coefs\n",
    "            pval_map_subjs[subject - 1, itask, ...] = pvals\n",
    "\n",
    "    # make coef_map relative to \"linear\" coef (bias=0, kappa=1), per subj/task\n",
    "    rng = np.random.RandomState(42)\n",
    "    if relative_to_b0_k1:\n",
    "\n",
    "        if tasks == \"all\":\n",
    "            subjs = df[\"subject\"].unique()\n",
    "            for subj in subjs:\n",
    "                linear_coef = coef_map[subj - 1, kappa_1_idx, bias_0_idx]\n",
    "                coef_map[subj - 1, ...] -= linear_coef\n",
    "\n",
    "                # don't make the b=0, k=1 cell zero for all subjs. Add some\n",
    "                # very tiny random noise to that cell only.\n",
    "                coef_map[subj - 1, kappa_1_idx, bias_0_idx] = rng.randn(1)[0] * 1e-5\n",
    "\n",
    "        else:\n",
    "            for itask, task in enumerate(tasks):\n",
    "                subjs = df[df[\"task\"] == task][\"subject\"].unique()\n",
    "                for subj in subjs:\n",
    "                    linear_coef = coef_map[subj - 1, itask, kappa_1_idx, bias_0_idx]\n",
    "                    coef_map[subj - 1, itask, ...] -= linear_coef\n",
    "\n",
    "                # don't make the b=0, k=1 cell zero for all subjs. Add some\n",
    "                # very tiny random noise to that cell only.\n",
    "                coef_map[subj - 1, itask, kappa_1_idx, bias_0_idx] = (\n",
    "                    rng.randn(1)[0] * 1e-5\n",
    "                )\n",
    "\n",
    "    return coef_map, pval_map_subjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T13:32:27.995755Z",
     "start_time": "2021-05-02T13:32:27.967514Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_neurometics_data(tasks, df, coef_map_list, pval_map_subjs_list, wilcoxon):\n",
    "    \"\"\"Get more data out of coef_map.\"\"\"\n",
    "    if tasks == \"all\":\n",
    "        tasks = [tasks]\n",
    "\n",
    "    # combine coef_maps\n",
    "    if len(coef_map_list) > 1:\n",
    "        print(\"combining maps (mean)\")\n",
    "        coef_map = np.stack(coef_map_list, axis=0).mean(axis=0)\n",
    "        pval_map_subjs = np.stack(pval_map_subjs_list, axis=0).mean(axis=0)\n",
    "    else:\n",
    "        coef_map = coef_map_list[0]\n",
    "        pval_map_subjs = pval_map_subjs_list[0]\n",
    "\n",
    "    # Check that coefficients in the map are normally distributed\n",
    "    # because that is the assumption of the one-sample t-test we want to run below\n",
    "    # could also use \"shapiro\":\n",
    "    # https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/\n",
    "    # however, see:\n",
    "    # https://stats.stackexchange.com/a/2498/148275\n",
    "    not_normal = []\n",
    "    for itask, task in enumerate(tasks):\n",
    "        if task == \"all\":\n",
    "            _dat = coef_map[...]\n",
    "        else:\n",
    "            _dat = coef_map[:, itask, ...]\n",
    "        res_not_normal = (\n",
    "            scipy.stats.normaltest(_dat, axis=0, nan_policy=\"omit\")[1] < 0.001\n",
    "        )\n",
    "        if res_not_normal.any():\n",
    "            not_normal.append({task: res_not_normal.sum()})\n",
    "    if len(not_normal) > 0:\n",
    "        print(f\"Normality does not hold for n cells: {not_normal}\")\n",
    "        if wilcoxon == \"auto\":\n",
    "            print(\"Using Wilcoxon\")\n",
    "            wilcoxon = True\n",
    "\n",
    "    if wilcoxon == \"auto\":\n",
    "        wilcoxon = False\n",
    "\n",
    "    # calculate pvalue map --> using 1-samp ttest over subj correlation coefs\n",
    "    if tasks[0] == \"all\":\n",
    "        tval_map = np.full((n, n), np.nan)\n",
    "        pval_map = np.full((n, n), np.nan)\n",
    "        idxs = df[\"subject\"].unique() - 1\n",
    "        data = coef_map[idxs, ...]\n",
    "\n",
    "        if not wilcoxon:\n",
    "            # Standard T-test\n",
    "            tval, pval = scipy.stats.ttest_1samp(\n",
    "                a=data, popmean=0, axis=0, nan_policy=\"raise\"\n",
    "            )\n",
    "        else:\n",
    "            # Wilcoxon\n",
    "            _, rows, cols = data.shape\n",
    "            tval = np.full((rows, cols), np.nan)\n",
    "            pval = np.full((rows, cols), np.nan)\n",
    "            for irow in range(rows):\n",
    "                for icol in range(cols):\n",
    "                    t, p = scipy.stats.wilcoxon(data[:, irow, icol])\n",
    "                    tval[irow, icol] = t\n",
    "                    pval[irow, icol] = p\n",
    "\n",
    "        tval_map[...] = tval\n",
    "        pval_map[...] = pval\n",
    "\n",
    "    else:\n",
    "        tval_map = np.full((len(tasks), n, n), np.nan)\n",
    "        pval_map = np.full((len(tasks), n, n), np.nan)\n",
    "        for itask, task in enumerate(tasks):\n",
    "            idxs = df[df[\"task\"] == task][\"subject\"].unique() - 1\n",
    "            data = coef_map[idxs, itask, ...]\n",
    "\n",
    "            if not wilcoxon:\n",
    "                # Standard T-test\n",
    "                tval, pval = scipy.stats.ttest_1samp(\n",
    "                    a=data, popmean=0, axis=0, nan_policy=\"raise\"\n",
    "                )\n",
    "            else:\n",
    "                # Wilcoxon\n",
    "                _, rows, cols = data.shape\n",
    "                tval = np.full((rows, cols), np.nan)\n",
    "                pval = np.full((rows, cols), np.nan)\n",
    "                for irow in range(rows):\n",
    "                    for icol in range(cols):\n",
    "                        t, p = scipy.stats.wilcoxon(data[:, irow, icol])\n",
    "                        tval[irow, icol] = t\n",
    "                        pval[irow, icol] = p\n",
    "\n",
    "            tval_map[itask, ...] = tval\n",
    "            pval_map[itask, ...] = pval\n",
    "\n",
    "    # get the subject specific maxima on each map\n",
    "    if tasks[0] == \"all\":\n",
    "        # subj x data, where data is (ycoord, xcoord, value) of maximum\n",
    "        subj_maxima = np.full((40, 3), np.nan)\n",
    "        subjs = df[\"subject\"].unique()\n",
    "        for subj in subjs:\n",
    "            data = coef_map[subj - 1, ...]\n",
    "            subj_maxima[subj - 1, ...] = np.array(\n",
    "                (*np.unravel_index(np.argmax(data), data.shape), np.max(data))\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        # subj x task x data, where data is (ycoord, xcoord, value) of maximum\n",
    "        subj_maxima = np.full((40, 4, 3), np.nan)\n",
    "        for itask, task in enumerate(tasks):\n",
    "            subjs = df[df[\"task\"] == task][\"subject\"].unique()\n",
    "            for subj in subjs:\n",
    "                data = coef_map[subj - 1, itask, ...]\n",
    "                subj_maxima[subj - 1, itask, ...] = np.array(\n",
    "                    (*np.unravel_index(np.argmax(data), data.shape), np.max(data))\n",
    "                )\n",
    "\n",
    "    # sanity check that data[0], data[1] are integer coords\n",
    "    a = np.isclose(subj_maxima - subj_maxima.astype(int), 0)\n",
    "    assert not a[..., -1].any()  # float maxima values\n",
    "    assert a[..., 0:2][~np.isnan(subj_maxima[..., 0:2])].all()  # int coords\n",
    "\n",
    "    return coef_map, pval_map_subjs, tval_map, pval_map, subj_maxima, wilcoxon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load EEG-RDM collapsed over tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T13:32:28.008080Z",
     "start_time": "2021-05-02T13:32:27.997623Z"
    }
   },
   "outputs": [],
   "source": [
    "preproc_dict = dict(\n",
    "    crop=(0.6, 1.6),\n",
    "    smooth=150,\n",
    "    tshift=-0.8,\n",
    "    baseline=(None, 0),\n",
    "    average=True,\n",
    "    crossvalidate=False,\n",
    "    smooth_before_baseline=True,\n",
    ")\n",
    "\n",
    "# make sure we are using the correct preproc settings\n",
    "for key, val in preproc_dict.items():\n",
    "    if key in [\"average\", \"crossvalidate\"]:\n",
    "        continue\n",
    "    assert key[0] + \"-\" + str(val) in rsa_results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T13:32:28.032690Z",
     "start_time": "2021-05-02T13:32:28.012195Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    overall_eeg_rdms\n",
    "    print(\"'overall_eeg_rdms' loaded, doing nothing.\")\n",
    "except NameError:\n",
    "    print(\"reading EEG RDMs over tasks\")\n",
    "    subjects = np.arange(1, 41).astype(int)\n",
    "    overall_eeg_rdms = np.full((len(subjects), 9, 9, len(times)), np.nan)\n",
    "    for subj in subjects:\n",
    "        # load epochs\n",
    "        fname = name_templates[\"epochs\"].format(subj)\n",
    "        epochs = mne.read_epochs(fname, preload=True, verbose=False)\n",
    "        epochs = epochs.pick_types(meg=False, eeg=True, verbose=False)\n",
    "\n",
    "        # Get an ERP for each number (applying preprocessing to epochs)\n",
    "        data_uV = np.full((9, 64, len(times)), np.nan)\n",
    "        for inumber in range(1, 10):\n",
    "            epochs_sel = epochs[f\"out{inumber}\"]\n",
    "            erp = prep_epochs(epochs_sel, **preproc_dict)\n",
    "\n",
    "            data_uV[inumber - 1, ...] = erp.data * 1e6\n",
    "\n",
    "        # Turn the numberwise ERPs into an RDM over time\n",
    "        nkeys, nchs, ntimes = data_uV.shape\n",
    "        rdm_times = np.full((nkeys, nkeys, ntimes), np.nan)\n",
    "\n",
    "        for itime in range(ntimes):\n",
    "            # get the 9x64 data\n",
    "            timepoint_data = data_uV[..., itime]\n",
    "\n",
    "            rdm_times[..., itime] = squareform(\n",
    "                pdist(timepoint_data, metric=\"euclidean\")\n",
    "            )\n",
    "\n",
    "        overall_eeg_rdms[subj - 1, ...] = rdm_times\n",
    "\n",
    "    # Sanity check_ we loaded the RDMs over time\n",
    "    assert overall_eeg_rdms.shape == (40, 9, 9, 251)\n",
    "    print(f\"collapsing RDMs over time using {max_cluster}\")\n",
    "\n",
    "    # Take mean of RDMs over time\n",
    "    overall_eeg_rdms = overall_eeg_rdms[..., max_cluster].mean(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:05:49.561137Z",
     "start_time": "2021-05-02T13:32:28.034931Z"
    }
   },
   "outputs": [],
   "source": [
    "for tasks_to_use in [tasks, \"all\"]:\n",
    "\n",
    "    if tasks_to_use == \"all\":\n",
    "        eeg_rdms_to_use = overall_eeg_rdms  # overall_eeg_rdms, eeg_rdms\n",
    "    else:\n",
    "        eeg_rdms_to_use = eeg_rdms\n",
    "\n",
    "    neurometrics_data = {}\n",
    "    for model_to_use in [\"numberline\", \"extremity\", \"both\"]:\n",
    "\n",
    "        generate_maps = False\n",
    "        if model_to_use == \"numberline\":\n",
    "            model_rdms = NUMBERLINE_RDMS\n",
    "            generate_maps = True\n",
    "        elif model_to_use == \"extremity\":\n",
    "            model_rdms = EXTREMITY_RDMS\n",
    "            generate_maps = True\n",
    "        elif model_to_use == \"both\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"unexpected model_to_use: {model_to_use}\")\n",
    "\n",
    "        if generate_maps:\n",
    "            coef_map, pval_map_subjs = generate_coef_maps(\n",
    "                tasks_to_use,\n",
    "                df,\n",
    "                model_rdms,\n",
    "                eeg_rdms_to_use,\n",
    "                corr_method,\n",
    "                bias_kappa_combis,\n",
    "                lower_triangle_idx,\n",
    "                do_orth,\n",
    "                orth_demean,\n",
    "                orth_other_model,\n",
    "                NJOBS,\n",
    "                relative_to_b0_k1=relative_to_b0_k1,\n",
    "                bias_0_idx=bias_0_idx,\n",
    "                kappa_1_idx=kappa_1_idx,\n",
    "            )\n",
    "\n",
    "            neurometrics_data[model_to_use] = (\n",
    "                coef_map,\n",
    "                pval_map_subjs,\n",
    "            )\n",
    "\n",
    "        # Generate data based on maps\n",
    "        if model_to_use == \"both\":\n",
    "            coef_map_list = []\n",
    "            pval_map_subjs_list = []\n",
    "            for _model in [\"numberline\", \"extremity\"]:\n",
    "                coef_map_list += [neurometrics_data[_model][0]]\n",
    "                pval_map_subjs_list += [neurometrics_data[_model][1]]\n",
    "        else:\n",
    "            coef_map_list = [neurometrics_data[model_to_use][0]]\n",
    "            pval_map_subjs_list = [neurometrics_data[model_to_use][1]]\n",
    "\n",
    "        (\n",
    "            coef_map,\n",
    "            pval_map_subjs,\n",
    "            tval_map,\n",
    "            pval_map,\n",
    "            subj_maxima,\n",
    "            wilcoxon,\n",
    "        ) = get_neurometics_data(\n",
    "            tasks_to_use, df, coef_map_list, pval_map_subjs_list, wilcoxon=wilcoxon\n",
    "        )\n",
    "\n",
    "        # save the data\n",
    "        neurometrics_data[model_to_use] = (\n",
    "            coef_map,\n",
    "            pval_map_subjs,\n",
    "            tval_map,\n",
    "            pval_map,\n",
    "            subj_maxima,\n",
    "            wilcoxon,\n",
    "        )\n",
    "\n",
    "    # pickle it\n",
    "    plotdir = os.path.join(BIDS_ROOT, \"code\", \"publication_plots\")\n",
    "    fname = os.path.join(\n",
    "        plotdir,\n",
    "        f\"nm_data_{'-'.join(tasks_to_use)}_{corr_method}_b-{biases.max()}_k-{kappas.max()}.pickle\",\n",
    "    )\n",
    "    savedict = dict(neurometrics_data)\n",
    "    savedict[\"bias_kappa_info\"] = (\n",
    "        bias_kappa_combis,\n",
    "        biases,\n",
    "        kappas,\n",
    "        bias_0_idx,\n",
    "        kappa_1_idx,\n",
    "    )\n",
    "    with open(fname, \"wb\") as fout:\n",
    "        pickle.dump(savedict, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T15:11:56.610394Z",
     "start_time": "2021-05-30T15:11:56.374974Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the saved results\n",
    "# ----------------------\n",
    "tasks_to_use = tasks  # \"all\" or tasks\n",
    "\n",
    "\n",
    "if tasks_to_use == \"all\":\n",
    "    eeg_rdms_to_use = overall_eeg_rdms\n",
    "else:\n",
    "    eeg_rdms_to_use = eeg_rdms\n",
    "\n",
    "plotdir = os.path.join(BIDS_ROOT, \"code\", \"publication_plots\")\n",
    "fname = os.path.join(\n",
    "    plotdir,\n",
    "    f\"nm_data_{'-'.join(tasks_to_use)}_{corr_method}_b-{biases.max()}_k-{kappas.max()}.pickle\",\n",
    ")\n",
    "with open(fname, \"rb\") as fin:\n",
    "    loaddict = pickle.load(fin)\n",
    "\n",
    "bias_kappa_combis, biases, kappas, bias_0_idx, kappa_1_idx = loaddict[\"bias_kappa_info\"]\n",
    "neurometrics_data = dict(loaddict)\n",
    "del neurometrics_data[\"bias_kappa_info\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:05:50.506850Z",
     "start_time": "2021-05-02T14:05:50.503116Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot settings\n",
    "# -------------\n",
    "step = int(n / 4)\n",
    "ticks = np.arange(0, n, step)\n",
    "ticks = np.append(ticks, n - 1)\n",
    "\n",
    "# for colorbar range (mean +- STD)\n",
    "std_scale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:05:57.641828Z",
     "start_time": "2021-05-02T14:05:50.508752Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks_to_use = list(np.atleast_1d(tasks_to_use))\n",
    "\n",
    "# extract data for one model\n",
    "figs = []\n",
    "highest_mean_params = {}\n",
    "for model_to_use in [\"numberline\", \"extremity\", \"both\"]:\n",
    "    (\n",
    "        coef_map,\n",
    "        pval_map_subjs,\n",
    "        tval_map,\n",
    "        pval_map,\n",
    "        subj_maxima,\n",
    "        wilcoxon,\n",
    "    ) = neurometrics_data[model_to_use]\n",
    "\n",
    "    # plot\n",
    "    fig, axs = plt.subplots(\n",
    "        len(tasks_to_use), 3, figsize=(10, 10), sharex=True, sharey=True\n",
    "    )\n",
    "    axs = np.atleast_2d(axs)\n",
    "\n",
    "    highest_mean_params[model_to_use] = {}\n",
    "    for itask, task in enumerate(tasks_to_use):\n",
    "\n",
    "        ax1, ax2, ax3 = axs[itask, :]\n",
    "\n",
    "        if task == \"all\":\n",
    "            _coefmap_to_plot = np.mean(coef_map, axis=0)\n",
    "            _tmap_to_plot = tval_map[...]\n",
    "            _pmap_to_plot = pval_map[...]\n",
    "        else:\n",
    "            _coefmap_to_plot = np.nanmean(coef_map[:, itask, ...], axis=0)\n",
    "            _tmap_to_plot = tval_map[itask, ...]\n",
    "            _pmap_to_plot = pval_map[itask, ...]\n",
    "\n",
    "        im1 = ax1.imshow(\n",
    "            _coefmap_to_plot,\n",
    "            # vmax=np.nanmean(coef_map) + std_scale * 2 * np.nanstd(coef_map),\n",
    "            # vmin=np.nanmean(coef_map) - std_scale * 2 * np.nanstd(coef_map),\n",
    "            origin=\"upper\",\n",
    "            interpolation=\"nearest\",\n",
    "        )\n",
    "\n",
    "        cmapmod = \"viridis_r\" if wilcoxon else \"viridis\"\n",
    "        im2 = ax2.imshow(\n",
    "            _tmap_to_plot,\n",
    "            # vmax=np.nanmean(tval_map) + std_scale * np.nanstd(tval_map),\n",
    "            # vmin=np.nanmean(tval_map) - std_scale * np.nanstd(tval_map),\n",
    "            origin=\"upper\",\n",
    "            interpolation=\"nearest\",\n",
    "            cmap=cmapmod,\n",
    "        )\n",
    "\n",
    "        norm_pvals = False\n",
    "        if norm_pvals:\n",
    "            # NOTE: rescaling (normalizing) p-values for visualization purposes\n",
    "            norm = matplotlib.colors.TwoSlopeNorm(vmin=0, vmax=1, vcenter=0.05)\n",
    "            _extra_kwargs = dict(cmap=\"RdBu_r\", norm=norm)\n",
    "            pvalmod = \"normalized \"\n",
    "        else:\n",
    "            pvalmod = \"\"\n",
    "            _extra_kwargs = dict()\n",
    "\n",
    "        im3 = ax3.imshow(\n",
    "            _pmap_to_plot,\n",
    "            # vmax=np.min([1, np.nanmean(pval_map) + std_scale * np.nanstd(pval_map)]),\n",
    "            vmax=0.1,\n",
    "            vmin=np.max([0, np.nanmean(pval_map) - std_scale * np.nanstd(pval_map)]),\n",
    "            origin=\"upper\",\n",
    "            interpolation=\"nearest\",\n",
    "            **_extra_kwargs,\n",
    "        )\n",
    "\n",
    "        # plot individual subj maxima\n",
    "        if task == \"all\":\n",
    "            ax1.scatter(\n",
    "                subj_maxima[..., 0:2][:, 1],\n",
    "                subj_maxima[..., 0:2][:, 0],\n",
    "                marker=\"o\",\n",
    "                facecolor=\"white\",\n",
    "                edgecolor=\"black\",\n",
    "                s=10,\n",
    "            )\n",
    "\n",
    "            # plot subj summary\n",
    "            kappa_idx, bias_idx = np.around(\n",
    "                np.mean(subj_maxima[..., 0:2], axis=0)\n",
    "            ).astype(int)\n",
    "\n",
    "        else:\n",
    "            subjs = df[df[\"task\"] == task][\"subject\"].unique()\n",
    "            ax1.scatter(\n",
    "                subj_maxima[subjs - 1, itask, 0:2][:, 1],\n",
    "                subj_maxima[subjs - 1, itask, 0:2][:, 0],\n",
    "                marker=\"o\",\n",
    "                facecolor=\"white\",\n",
    "                edgecolor=\"black\",\n",
    "                s=10,\n",
    "            )\n",
    "\n",
    "            # plot subj summary\n",
    "            kappa_idx, bias_idx = np.around(\n",
    "                np.mean(subj_maxima[subjs - 1, itask, 0:2], axis=0)\n",
    "            ).astype(int)\n",
    "\n",
    "        ax1.scatter(\n",
    "            bias_idx,\n",
    "            kappa_idx,\n",
    "            marker=\"d\",\n",
    "            facecolor=\"white\",\n",
    "            edgecolor=\"black\",\n",
    "            s=50,\n",
    "        )\n",
    "\n",
    "        # save maximum values\n",
    "        highest_mean_params[model_to_use][task] = (biases[bias_idx], kappas[kappa_idx])\n",
    "\n",
    "        cbar1 = fig.colorbar(im1, ax=ax1)\n",
    "        cbar1.ax.set_ylabel(f\"{corr_method} correlation\")\n",
    "        tstatmod = \"wilcoxon\" if wilcoxon else \"t\"\n",
    "        cbar2 = fig.colorbar(im2, ax=ax2)\n",
    "        cbar2.ax.set_ylabel(f\"test statistic ({tstatmod})\")\n",
    "        cbar3 = fig.colorbar(im3, ax=ax3)\n",
    "        cbar3.ax.set_ylabel(f\"{pvalmod}p-value\")\n",
    "\n",
    "        # plot \"cross\" at bias 0 and kappa 1\n",
    "        ax1.axvline(bias_0_idx, color=\"white\", linestyle=\"--\")\n",
    "        ax1.axhline(kappa_1_idx, color=\"white\", linestyle=\"--\")\n",
    "        ax2.axvline(bias_0_idx, color=\"white\", linestyle=\"--\")\n",
    "        ax2.axhline(kappa_1_idx, color=\"white\", linestyle=\"--\")\n",
    "        ax3.axvline(bias_0_idx, color=\"white\", linestyle=\"--\")\n",
    "        ax3.axhline(kappa_1_idx, color=\"white\", linestyle=\"--\")\n",
    "\n",
    "        # settings for ax1 will be applied to ax2 due to sharex, sharey\n",
    "        ax1.set(\n",
    "            title=task,\n",
    "            xticks=ticks,\n",
    "            xticklabels=biases[ticks].round(2),\n",
    "            xlabel=\"Bias (b)\",\n",
    "            yticks=ticks,\n",
    "            yticklabels=kappas[ticks].round(2),\n",
    "            ylabel=\"Kappa (k)\",\n",
    "        )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    suptitle = fig.suptitle(model_to_use, y=1.01)\n",
    "    figs += [fig]\n",
    "\n",
    "    # save plot\n",
    "    neurometrics_dir = os.path.join(BIDS_ROOT, \"derivatives\", \"neurometrics\")\n",
    "    os.makedirs(neurometrics_dir, exist_ok=True)\n",
    "    plotsavedir = \"\"\n",
    "    plotsavedir += f\"corr-{corr_method}_orth-{do_orth}_demean-{orth_demean}\"\n",
    "    plotsavedir += f\"_kmax-{kappas.max()}_bmax-{biases.max()}\"\n",
    "    plotsavedir = os.path.join(neurometrics_dir, plotsavedir)\n",
    "    os.makedirs(plotsavedir, exist_ok=True)\n",
    "    if plotsave:\n",
    "        fpath = os.path.join(plotsavedir, f\"neurometric_maps_{model_to_use}.pdf\")\n",
    "        fig.savefig(fpath, bbox_extra_artists=[suptitle], bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:05:59.278488Z",
     "start_time": "2021-05-02T14:05:57.643073Z"
    }
   },
   "outputs": [],
   "source": [
    "highest_mean_params_copy = dict(highest_mean_params)\n",
    "del highest_mean_params_copy[\"both\"]\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    len(highest_mean_params_copy.keys()),\n",
    "    len(highest_mean_params_copy[list(highest_mean_params_copy.keys())[0]].values()),\n",
    "    figsize=(10, 8),\n",
    ")\n",
    "\n",
    "for imodel, model_to_use in enumerate(highest_mean_params_copy):\n",
    "\n",
    "    combis_to_plot = list(highest_mean_params_copy[model_to_use].values())\n",
    "    _model_rdms = produce_model_rdms(model_to_use, combis_to_plot)\n",
    "\n",
    "    # Plot model-RDMs\n",
    "    # ---------------\n",
    "    for i, ax in enumerate(axs[imodel, :].flat):\n",
    "\n",
    "        _to_plot = _model_rdms[..., i].copy()\n",
    "\n",
    "        # Remove upper triangle\n",
    "        tri_idx = np.triu_indices(_to_plot.shape[0])\n",
    "        _to_plot[tri_idx] = np.nan\n",
    "\n",
    "        im = ax.imshow(_to_plot)\n",
    "\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        bias, kappa = combis_to_plot[i]\n",
    "        ax.set(\n",
    "            title=f\"{(list(tasks)*2)[i]}\\n{model_to_use}\\nb={bias:.2f}\\nk={kappa:.2f}\"\n",
    "        )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "if plotsave:\n",
    "    fpath = os.path.join(plotsavedir, \"neurometric_best_models.pdf\")\n",
    "    fig.savefig(fpath, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2x2 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:05:59.395572Z",
     "start_time": "2021-05-02T14:05:59.280198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect subject wide maxima for bias and kappa ... for each \"model\"\n",
    "dfs = []\n",
    "for model_to_use in [\"numberline\", \"extremity\", \"both\"]:\n",
    "    (\n",
    "        coef_map,\n",
    "        pval_map_subjs,\n",
    "        tval_map,\n",
    "        pval_map,\n",
    "        subj_maxima,\n",
    "        wilcoxon,\n",
    "    ) = neurometrics_data[model_to_use]\n",
    "\n",
    "    _dfs = []\n",
    "    for itask, task in enumerate(tasks):\n",
    "        data = {}\n",
    "\n",
    "        subj_idxs = np.unique(df[df[\"task\"] == task][\"subject\"]) - 1\n",
    "        param_idxs = subj_maxima[subj_idxs, itask, ...][:, 0:2]\n",
    "\n",
    "        data[\"kappa\"] = kappas[param_idxs[:, 0].astype(int)]\n",
    "        data[\"bias\"] = biases[param_idxs[:, 1].astype(int)]\n",
    "        data[\"subject\"] = subj_idxs + 1\n",
    "\n",
    "        _df = pd.DataFrame(data)\n",
    "        _df[\"task\"] = task\n",
    "        _dfs.append(_df)\n",
    "\n",
    "    _df = pd.concat(_dfs)[[\"subject\", \"task\", \"bias\", \"kappa\"]]\n",
    "    _df[\"sampling\"] = _df[\"task\"].str[0].map({\"A\": \"active\", \"Y\": \"yoked\"})\n",
    "    _df[\"stopping\"] = _df[\"task\"].str[1].map({\"V\": \"variable\", \"F\": \"fixed\"})\n",
    "    _df[\"model\"] = model_to_use\n",
    "\n",
    "    assert not _df.isna().any().any()\n",
    "\n",
    "    dfs.append(_df)\n",
    "\n",
    "_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:05:59.401259Z",
     "start_time": "2021-05-02T14:05:59.396699Z"
    }
   },
   "outputs": [],
   "source": [
    "def plt_single_subj(ax, order, hue_order, data, colname):\n",
    "    subj_line_settings = dict(color=\"black\", alpha=0.1, linewidth=0.75)\n",
    "    _idxs_to_pick = [(2, 4), (3, 5)]\n",
    "    for i_dots, (idx0, idx1) in enumerate(_idxs_to_pick):\n",
    "        locs1 = ax.get_children()[idx0].get_offsets()\n",
    "        locs2 = ax.get_children()[idx1].get_offsets()\n",
    "\n",
    "        # Need to sort locs, so data corresponds\n",
    "        sort_idxs_list = []\n",
    "        sampling = order[i_dots]\n",
    "        for stopping in hue_order:\n",
    "            arr = data[(data[\"sampling\"] == sampling) & (data[\"stopping\"] == stopping)][\n",
    "                colname\n",
    "            ].to_numpy()\n",
    "            sort_idxs_list += [np.argsort(arr)]\n",
    "\n",
    "        locs2_sorted = locs2[sort_idxs_list[1].argsort()][sort_idxs_list[0]]\n",
    "\n",
    "        for i in range(locs1.shape[0]):\n",
    "            _x = [locs1[i, 0], locs2_sorted[i, 0]]\n",
    "            _y = [locs1[i, 1], locs2_sorted[i, 1]]\n",
    "            ax.plot(_x, _y, **subj_line_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:06:01.385683Z",
     "start_time": "2021-05-02T14:05:59.402804Z"
    }
   },
   "outputs": [],
   "source": [
    "models_to_use = [\"numberline\", \"extremity\", \"both\"]\n",
    "fig, axs = plt.subplots(\n",
    "    len(models_to_use), 2, figsize=(10, 15), sharex=True, sharey=False\n",
    ")\n",
    "\n",
    "for imodel, model_to_use in enumerate(models_to_use):\n",
    "\n",
    "    ax1, ax2 = axs[imodel, :]\n",
    "\n",
    "    _data = _df[_df[\"model\"] == model_to_use]\n",
    "\n",
    "    kwargs = dict(\n",
    "        dodge=True,\n",
    "        data=_data,\n",
    "        x=\"sampling\",\n",
    "        hue=\"stopping\",\n",
    "        order=[\"active\", \"yoked\"],\n",
    "        hue_order=[\"fixed\", \"variable\"],\n",
    "    )\n",
    "\n",
    "    sns.pointplot(y=\"bias\", ax=ax1, ci=68, **kwargs)\n",
    "    sns.swarmplot(y=\"bias\", ax=ax1, **kwargs, size=3)\n",
    "\n",
    "    plt_single_subj(ax1, kwargs[\"order\"], kwargs[\"hue_order\"], _data, \"bias\")\n",
    "\n",
    "    sns.pointplot(y=\"kappa\", ax=ax2, ci=68, **kwargs)\n",
    "    sns.stripplot(y=\"kappa\", ax=ax2, **kwargs, size=3)\n",
    "\n",
    "    plt_single_subj(ax2, kwargs[\"order\"], kwargs[\"hue_order\"], _data, \"kappa\")\n",
    "\n",
    "    ax1.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "    ax2.axhline(1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set(\n",
    "        xticklabels=[\"self-\\ncontrolled\", \"yoked\"],\n",
    "        ylim=(-1.1, 1.1),\n",
    "        title=model_to_use + \" (bias)\",\n",
    "    )\n",
    "    ax2.set(title=model_to_use + \" (kappa)\")\n",
    "\n",
    "    ax2.get_legend().remove()\n",
    "    if imodel == 0:\n",
    "        handles, labels = ax1.get_legend_handles_labels()\n",
    "        leg = ax1.legend(\n",
    "            handles[0:2],\n",
    "            [{\"fixed\": \"partial\", \"variable\": \"full\"}[i] for i in labels[0:2]],\n",
    "            ncol=2,\n",
    "            bbox_to_anchor=(0.25, 0.96),\n",
    "            bbox_transform=fig.transFigure,\n",
    "        )\n",
    "    else:\n",
    "        ax1.get_legend().remove()\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "if plotsave:\n",
    "    fpath = os.path.join(plotsavedir, \"neurometric_2x2.pdf\")\n",
    "    fig.savefig(fpath, bbox_inches=\"tight\", bbox_extra_artists=[leg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:06:01.790811Z",
     "start_time": "2021-05-02T14:06:01.387126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate mixed ANOVAs for the 2x2 params, and save the tables as HTML file\n",
    "\n",
    "if plotsave:\n",
    "\n",
    "    with open(os.path.join(plotsavedir, \"neurometric_2x2_stats.html\"), \"w\") as fout:\n",
    "\n",
    "        for param in [\"bias\", \"kappa\"]:\n",
    "            print(\"<p></p>\" + param + \"<p></p>\", file=fout)\n",
    "\n",
    "            for model_to_use in models_to_use:\n",
    "                _data = _df[_df[\"model\"] == model_to_use]\n",
    "                stats = pingouin.mixed_anova(\n",
    "                    data=_data,\n",
    "                    dv=param,\n",
    "                    within=\"sampling\",\n",
    "                    between=\"stopping\",\n",
    "                    subject=\"subject\",\n",
    "                )\n",
    "\n",
    "                to_write = stats.to_html()\n",
    "\n",
    "                print(model_to_use, file=fout)\n",
    "                print(to_write, file=fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot neurometric function over rescaled numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:06:04.661236Z",
     "start_time": "2021-05-02T14:06:01.792315Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    len(models_to_use), 2, figsize=(8, 8), sharex=True, sharey=False\n",
    ")\n",
    "\n",
    "n_points_X = 100\n",
    "\n",
    "for imodel, model_to_use in enumerate(models_to_use):\n",
    "\n",
    "    ax1, ax2 = axs[imodel, :]\n",
    "\n",
    "    _data = _df[_df[\"model\"] == model_to_use]\n",
    "\n",
    "    ax1.plot(np.linspace(-1, 1, 3), np.linspace(-1, 1, 3), \"k-\")\n",
    "    ax1.axhline(0, color=\"black\", linewidth=0.1)\n",
    "    ax1.axvline(0, color=\"black\", linewidth=0.1)\n",
    "\n",
    "    # Plot task-wise data\n",
    "    for task, grp in _data.groupby(\"task\"):\n",
    "\n",
    "        linestyle = \"--\" if \"Y\" in task else \"-\"\n",
    "        color = sns.color_palette()[0] if \"F\" in task else sns.color_palette()[1]\n",
    "\n",
    "        bias = grp.mean()[\"bias\"]\n",
    "        kappa = grp.mean()[\"kappa\"]\n",
    "        values = eq1(\n",
    "            np.linspace(-1, 1, n_points_X),\n",
    "            bias=bias,\n",
    "            kappa=kappa,\n",
    "        )\n",
    "\n",
    "        if model_to_use == \"extremity\":\n",
    "            values = np.abs(values)\n",
    "        else:\n",
    "            assert model_to_use in [\"numberline\", \"both\"]\n",
    "\n",
    "        ax1.plot(\n",
    "            np.linspace(-1, 1, n_points_X),\n",
    "            values,\n",
    "            label=f\"{task}, b={bias:.2f}, k={kappa:.2f}\",\n",
    "            color=color,\n",
    "            linestyle=linestyle,\n",
    "        )\n",
    "\n",
    "        if model_to_use != \"both\":\n",
    "            # plot the \"markers\" on top of the lines\n",
    "            values = eq1(\n",
    "                numbers_rescaled,\n",
    "                bias=bias,\n",
    "                kappa=kappa,\n",
    "            )\n",
    "\n",
    "            if model_to_use == \"extremity\":\n",
    "                values = np.abs(values)\n",
    "\n",
    "            ax1.plot(\n",
    "                numbers_rescaled,\n",
    "                values,\n",
    "                color=color,\n",
    "                marker=\"o\",\n",
    "                linestyle=\"\",\n",
    "                markersize=3,\n",
    "            )\n",
    "\n",
    "    # plot summarized over tasks\n",
    "    bias = _data.mean()[\"bias\"]\n",
    "    kappa = _data.mean()[\"kappa\"]\n",
    "    values = eq1(\n",
    "        np.linspace(-1, 1, n_points_X),\n",
    "        bias=bias,\n",
    "        kappa=kappa,\n",
    "    )\n",
    "\n",
    "    if model_to_use == \"extremity\":\n",
    "        values = np.abs(values)\n",
    "\n",
    "    ax2.plot(np.linspace(-1, 1, 3), np.linspace(-1, 1, 3), \"k-\")\n",
    "    ax2.axhline(0, color=\"black\", linewidth=0.1)\n",
    "    ax2.axvline(0, color=\"black\", linewidth=0.1)\n",
    "\n",
    "    ax2.plot(\n",
    "        np.linspace(-1, 1, n_points_X),\n",
    "        values,\n",
    "        \"r-\",\n",
    "        label=f\"(mean), b={bias:.2f}, k={kappa:.2f}\",\n",
    "    )\n",
    "\n",
    "    # plot the \"markers\" on top of the lines\n",
    "    values = eq1(\n",
    "        numbers_rescaled,\n",
    "        bias=bias,\n",
    "        kappa=kappa,\n",
    "    )\n",
    "\n",
    "    if model_to_use == \"extremity\":\n",
    "        values = np.abs(values)\n",
    "\n",
    "    ax2.plot(\n",
    "        numbers_rescaled,\n",
    "        values,\n",
    "        \"ro\",\n",
    "        linestyle=\"\",\n",
    "        markersize=3,\n",
    "    )\n",
    "\n",
    "    # Tweaks\n",
    "    # ------\n",
    "    ax1.legend(title=\"Task and parameters\")\n",
    "    ax2.legend(title=\"Parameters\")\n",
    "    ax1.set(\n",
    "        title=model_to_use,\n",
    "        xlabel=\"X\",\n",
    "        ylabel=\"Decision weight\",\n",
    "    )\n",
    "    ax2.set(\n",
    "        title=model_to_use,\n",
    "        xlabel=\"X\",\n",
    "        ylabel=\"Decision weight\",\n",
    "    )\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "if plotsave:\n",
    "    fpath = os.path.join(plotsavedir, \"neurometric_curves.pdf\")\n",
    "    fig.savefig(fpath, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot model RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:06:04.667263Z",
     "start_time": "2021-05-02T14:06:04.662568Z"
    }
   },
   "outputs": [],
   "source": [
    "model_to_use = \"extremity\"\n",
    "\n",
    "combis_to_plot = list(\n",
    "    itertools.product(np.linspace(-1, 1, 3), np.linspace(0.5, 2.5, 5))\n",
    ")\n",
    "ncombis = len(combis_to_plot)\n",
    "\n",
    "_model_rdms = produce_model_rdms(model_to_use, combis_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:06:06.449880Z",
     "start_time": "2021-05-02T14:06:04.670231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot model-RDMs\n",
    "# ---------------\n",
    "fig, axs = plt.subplots(\n",
    "    len(np.unique(np.array(combis_to_plot)[:, 0])),\n",
    "    len(np.unique(np.array(combis_to_plot)[:, 1])),\n",
    "    figsize=(10, 10),\n",
    ")\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "\n",
    "    try:\n",
    "        im = ax.imshow(_model_rdms[..., i])\n",
    "    except IndexError:\n",
    "        ax.axis(\"off\")\n",
    "        continue\n",
    "\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    bias, kappa = combis_to_plot[i]\n",
    "    ax.set(title=f\"b={bias:.2f}\\nk={kappa:.2f}\")\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "suptitle = fig.suptitle(model_to_use, y=1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Eq1 (kappa & bias)\n",
    "\n",
    "notes for numberline:\n",
    "\n",
    "decision weight: distance to zero\n",
    "\n",
    "- Kappa is the compression parameter\n",
    "    - if 1: linear\n",
    "    - if < 1: compression: outliers are \"compressed, relative to \"inliers\"\n",
    "    - if > 1: anti-compression: opposite of compression\n",
    "    - always with respect to the midpoint of the scale (0)\n",
    "- Bias is an offset parameter\n",
    "    - can move the \"inflection point\" of the decision weights to the left or right of the scale\n",
    "    - bias 0: inflection point is at the midpoint of the scale\n",
    "    - bias > 1: inflection point moves left, high `X` gain more weight then low `X`\n",
    "    - bias < 1: inflection point moves right, low `X` gain more weight then high `X`\n",
    "    \n",
    "    \n",
    "kappa and bias together inform about the point of \"highest sensitivity\" (where the slope / rate of change is highest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T13:03:10.970694Z",
     "start_time": "2021-05-02T13:03:09.503947Z"
    }
   },
   "outputs": [],
   "source": [
    "# numberline\n",
    "cmap = sns.color_palette(\"crest_r\", as_cmap=True)\n",
    "\n",
    "bs = np.linspace(-1, 1, 7)\n",
    "ks = np.linspace(0.4, 3, 5)\n",
    "\n",
    "fig, axs = plt.subplots(1, len(bs), figsize=(10, 5), sharex=True, sharey=True)\n",
    "for i, b in enumerate(bs):\n",
    "    ax = axs.flat[i]\n",
    "    ax.plot(\n",
    "        np.linspace(-1, 1, 9),\n",
    "        eq1(numbers_rescaled, bias=0, kappa=1),\n",
    "        color=\"k\",\n",
    "        marker=\"o\",\n",
    "        label=\"b=0, k=1\",\n",
    "    )\n",
    "    ax.set(title=f\"bias={b:.2}\", ylabel=\"decision weight\", xlabel=\"X\")\n",
    "\n",
    "    for k in ks:\n",
    "        ax.plot(\n",
    "            np.linspace(-1, 1, 1000),\n",
    "            eq1(np.linspace(-1, 1, 1000), b, k),\n",
    "            color=cmap(k / ks.max()),\n",
    "            label=f\"k={k:.2f}\",\n",
    "        )\n",
    "\n",
    "    ax.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "    ax.axvline(0, color=\"k\", linestyle=\"--\")\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "fig.suptitle(\"Numberline\", y=1.02)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T13:03:11.942915Z",
     "start_time": "2021-05-02T13:03:10.971855Z"
    }
   },
   "outputs": [],
   "source": [
    "# extremity\n",
    "cmap = sns.color_palette(\"crest_r\", as_cmap=True)\n",
    "\n",
    "bs = np.linspace(-1, 1, 7)\n",
    "ks = np.array([0.5, 1, 1.5])\n",
    "\n",
    "fig, axs = plt.subplots(1, len(bs), figsize=(10, 5), sharex=True, sharey=True)\n",
    "for i, b in enumerate(bs):\n",
    "    ax = axs.flat[i]\n",
    "    ax.plot(\n",
    "        np.linspace(-1, 1, 9),\n",
    "        np.abs(eq1(numbers_rescaled, bias=0, kappa=1)),  # NOTE: np.abs !\n",
    "        color=\"k\",\n",
    "        marker=\"o\",\n",
    "        label=\"b=0, k=1\",\n",
    "    )\n",
    "    ax.set(title=f\"bias={b:.2}\", ylabel=\"decision weight\", xlabel=\"X\")\n",
    "\n",
    "    for k in ks:\n",
    "        ax.plot(\n",
    "            np.linspace(-1, 1, 1000),\n",
    "            np.abs(eq1(np.linspace(-1, 1, 1000), b, k)),  # NOTE: np.abs !\n",
    "            color=cmap(k / ks.max()),\n",
    "            label=f\"k={k:.2f}\",\n",
    "        )\n",
    "\n",
    "    ax.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "    ax.axvline(0, color=\"k\", linestyle=\"--\")\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "fig.suptitle(\"Extremity\", y=1.02)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "395.967px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
